{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NeuMF V2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"code","metadata":{"id":"EMFc_rrpq0ss"},"source":["import numpy as np\n","import pandas as pd\n","from util import *\n","from model import MLPEmbedding\n","from model import GMF\n","from evaluate import *\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BS7SdOI3q2MZ"},"source":["train_data = pd.read_pickle(\"../Data/train_data.pkl\")\n","val_data = pd.read_pickle(\"../Data/val_data.pkl\")\n","test_data = pd.read_pickle(\"../Data/test_data.pkl\")[['review_profilename','beer_name','review_overall']]\n","test_data = merge_user_id(test_data, on='review_profilename').rename(columns={'id':'user_id'})\n","test_data = merge_beer_id(test_data, on='beer_name').rename(columns={'id':'beer_id'})[['user_id','beer_id','review_overall']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"SfHpw65IlJ_o","outputId":"8654ea2a-9eda-494a-fd4c-c3c7bc259a50"},"source":["# convert review score to boolean - if the item is relevant to the user (rating >= 3.5) then 1, else 0\n","threshold = 4.0\n","train_data['relevant'] = (train_data['review_overall'] >= threshold).astype(int)\n","train_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>beer_id</th>\n","      <th>review_overall</th>\n","      <th>relevant</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>383941</th>\n","      <td>415</td>\n","      <td>831</td>\n","      <td>3.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>483700</th>\n","      <td>2492</td>\n","      <td>3491</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>375376</th>\n","      <td>1233</td>\n","      <td>1178</td>\n","      <td>4.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>705138</th>\n","      <td>1230</td>\n","      <td>902</td>\n","      <td>4.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>435972</th>\n","      <td>2290</td>\n","      <td>7497</td>\n","      <td>3.5</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        user_id  beer_id  review_overall  relevant\n","383941      415      831             3.5         0\n","483700     2492     3491             4.0         1\n","375376     1233     1178             4.5         1\n","705138     1230      902             4.5         1\n","435972     2290     7497             3.5         0"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxzr2MIilLQ6","outputId":"45fbfb9e-1c6e-4723-de4f-dc42b1299bce"},"source":["### unique users & beers in train_data\n","n_users = len(train_data['user_id'].unique())\n","n_beers = len(train_data['beer_id'].unique())\n","print(\"unique users: \", n_users)\n","print(\"unique beers: \", n_beers)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["unique users:  14811\n","unique beers:  52583\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xpqp9HK6q58l"},"source":["class NeuMF(nn.Module):\n","  def __init__(self, gmf_model, mlp_model, n_input, alpha=0.5, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n","    ## n_input = sum of output shape of the last layer of gmf and mlp model\n","    super().__init__()\n","    self.gmf_model=gmf_model\n","    self.mlp_model=mlp_model\n","    self.device = device\n","    print('n_input',n_input)\n","    self.net = nn.Linear(30, 1).to(self.device) \n","    \n","  \n","  def forward(self, df):\n","    # df: user_id, beer_id\n","#     df = torch.Tensor(df.to_numpy()).to(self.device)\n","    mlp_out = self.mlp_model.forward(df) # (n,25)\n","    ## TODO: output of the gmf_model\n","    gmf_out = self.gmf_model.forward_no_h(df)\n","#     print('gmf_out.size()', gmf_out.size())\n","#     print('mlp_out.size()', mlp_out.size())\n","    input = torch.cat((alpha*mlp_out, (1-alpha)*gmf_out),1).to(self.device)\n","#     print('input.size()', input.size())\n","    return self.net(input)\n","\n","  def predict(self, df):\n","    return torch.sigmoid(self.forward(df))\n","\n","  def loss(self, df, loss_fn):\n","    y_pred = self.forward(df).view(-1)\n","    y_train = torch.Tensor(df.relevant.to_numpy()).to(self.device)\n","    \n","    return loss_fn(y_pred, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYL4VnXTzUCl"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","n_hidden = 150\n","epochs = 40\n","batch_size = 500\n","lr = 0.01\n","loss_fn = nn.BCEWithLogitsLoss()\n","label_pred = 'pred_y'\n","k=5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5xsGR6WyvfM","outputId":"99bbb0b1-6c12-42b5-c67b-25f6905e0398"},"source":["# https://discuss.pytorch.org/t/merging-two-models/45637\n","gmf_model =  GMF(n_users,n_beers)\n","gmf_model.load_state_dict(torch.load(\"../Models/checkpoints/gmf.pth\"))\n","gmf_model.to(device)\n","mlp_model = MLPEmbedding(n_users=n_users, n_beers=n_beers, device=device, hidden_size=150).to(device)\n","mlp_model.load_state_dict(torch.load(\"../Models/checkpoints/mlp_best.pth\")['model_state_dict'])\n","mlp_model.to(device)\n","# neu_mf_model = NeuMF(gmf_model, mlp_model, n_input, device)\n","# optimizer = optim.Adam(list(neu_mf_model.gmf_model.parameters()) + list(neu_mf_model.mlp_model.parameters()) + list(neu_mf_model.net.parameters()), lr=1e-4, weight_decay=5e-4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPEmbedding(\n","  (users_emb): Embedding(14811, 150)\n","  (beers_emb): Embedding(52583, 150)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=300, out_features=200, bias=True)\n","    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout(p=0.6, inplace=False)\n","    (4): Linear(in_features=200, out_features=100, bias=True)\n","    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU()\n","    (7): Dropout(p=0.6, inplace=False)\n","    (8): Linear(in_features=100, out_features=50, bias=True)\n","    (9): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU()\n","    (11): Dropout(p=0.6, inplace=False)\n","    (12): Linear(in_features=50, out_features=25, bias=True)\n","    (13): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (14): ReLU()\n","    (15): Dropout(p=0.6, inplace=False)\n","  )\n","  (last_layer): Linear(in_features=25, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Loop1j8xREGc"},"source":["neu_mf_model.device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCcmnGP33WZJ","outputId":"b5656d8b-3a4b-44c9-a35c-60f3d4f52899"},"source":["import math\n","\n","# device = gpu\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","## model params\n","mlp_hidden_size = 150\n","gmf_hidden_size = 5\n","epochs = 40\n","batch_size = 500\n","lr = 0.001\n","loss_fn = nn.BCEWithLogitsLoss()\n","label_pred = 'pred_y'\n","alpha=0.6\n","k=5\n","\n","# n_users, n_beers, mlp_hidden_size, gmf_hidden_size, device\n","neu_mf_model = NeuMF(gmf_model, mlp_model, mlp_hidden_size, device)\n","optimizer = optim.Adam(list(neu_mf_model.gmf_model.parameters()) + list(neu_mf_model.mlp_model.parameters()) + list(neu_mf_model.net.parameters()), lr=1e-4, weight_decay=5e-4)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [5,10,20], gamma=0.5)\n","\n","l = []\n","val_p = []\n","train_p = []\n","best_p = 0\n","\n","for i in range(epochs):\n","    print(\"epoch: \",i)\n","    s = 0\n","    for bid in range(len(train_data) // batch_size):\n","        if (bid % 300 == 0):\n","          print(\"iteration: \",i, \", batch: \", bid)\n","        data = train_data[bid * batch_size : (bid + 1) * batch_size]\n","        optimizer.zero_grad()\n","        \n","        loss = neu_mf_model.loss(data, loss_fn)\n","        loss.backward()\n","        optimizer.step()\n","        s += loss\n","        \n","    scheduler.step()\n","    l.append(s.item()/(len(data) // batch_size))  \n","\n","    # evaluate precision at 10 of the model\n","  \n","    neu_mf_model.eval()\n","    with torch.no_grad():\n","      val = val_data.copy()\n","      val[label_pred] = (neu_mf_model.predict(val).cpu().detach().numpy() > 0.5).astype(int)\n","      val_prec, _ = precision_recall_at_k(val, label_pred=label_pred, threshold=threshold, k=k)\n","      val_prec = sum(prec for prec in val_prec.values()) / len(val_prec)\n","      train = train_data.copy()\n","      train[label_pred] = (neu_mf_model.predict(train).cpu().detach().numpy() > 0.5).astype(int)\n","      train_prec, _ = precision_recall_at_k(train, label_pred=label_pred, threshold=threshold, k=k)\n","      train_prec = sum(prec for prec in train_prec.values()) / len(train_prec)\n","\n","      train_p.append(train_prec)\n","      val_p.append(val_prec)\n","    neu_mf_model.train()\n","    \n","    print(\"Train precision at 5: \", train_prec)\n","    print(\"Validation precision at 5: \", val_prec)\n","    print(\"Current best validation precision at 5: \", best_p)\n","    if val_prec > best_p:\n","      print(\"Validation precision better than best current precision. Saving model to best_state_dict...\")\n","      best_state_dict = {\n","           'model_state_dict': neu_mf_model.state_dict()\n","           }\n","      best_p = val_prec\n","\n","      \n","    print(\"Average Loss for the current iteration: \", l[i])\n","    print(\"-----------------------------------\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["n_input 150\n","epoch:  0\n","iteration:  0 , batch:  0\n","iteration:  0 , batch:  300\n","iteration:  0 , batch:  600\n","iteration:  0 , batch:  900\n","iteration:  0 , batch:  1200\n","iteration:  0 , batch:  1500\n","iteration:  0 , batch:  1800\n","Train precision at 5:  0.7453514279927166\n","Validation precision at 5:  0.6907636216325881\n","Current best validation precision at 5:  0\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  1030.6793212890625\n","-----------------------------------\n","epoch:  1\n","iteration:  1 , batch:  0\n","iteration:  1 , batch:  300\n","iteration:  1 , batch:  600\n","iteration:  1 , batch:  900\n","iteration:  1 , batch:  1200\n","iteration:  1 , batch:  1500\n","iteration:  1 , batch:  1800\n","Train precision at 5:  0.7655031170526461\n","Validation precision at 5:  0.6917932617649202\n","Current best validation precision at 5:  0.6907636216325881\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  936.5554809570312\n","-----------------------------------\n","epoch:  2\n","iteration:  2 , batch:  0\n","iteration:  2 , batch:  300\n","iteration:  2 , batch:  600\n","iteration:  2 , batch:  900\n","iteration:  2 , batch:  1200\n","iteration:  2 , batch:  1500\n","iteration:  2 , batch:  1800\n","Train precision at 5:  0.7769056782121426\n","Validation precision at 5:  0.6910303153062072\n","Current best validation precision at 5:  0.6917932617649202\n","Average Loss for the current iteration:  902.0938720703125\n","-----------------------------------\n","epoch:  3\n","iteration:  3 , batch:  0\n","iteration:  3 , batch:  300\n","iteration:  3 , batch:  600\n","iteration:  3 , batch:  900\n","iteration:  3 , batch:  1200\n","iteration:  3 , batch:  1500\n","iteration:  3 , batch:  1800\n","Train precision at 5:  0.7870242837530663\n","Validation precision at 5:  0.6924189228726562\n","Current best validation precision at 5:  0.6917932617649202\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  881.3693237304688\n","-----------------------------------\n","epoch:  4\n","iteration:  4 , batch:  0\n","iteration:  4 , batch:  300\n","iteration:  4 , batch:  600\n","iteration:  4 , batch:  900\n","iteration:  4 , batch:  1200\n","iteration:  4 , batch:  1500\n","iteration:  4 , batch:  1800\n","Train precision at 5:  0.7890104201831972\n","Validation precision at 5:  0.6916300947494147\n","Current best validation precision at 5:  0.6924189228726562\n","Average Loss for the current iteration:  869.2230224609375\n","-----------------------------------\n","epoch:  5\n","iteration:  5 , batch:  0\n","iteration:  5 , batch:  300\n","iteration:  5 , batch:  600\n","iteration:  5 , batch:  900\n","iteration:  5 , batch:  1200\n","iteration:  5 , batch:  1500\n","iteration:  5 , batch:  1800\n","Train precision at 5:  0.8056602075034298\n","Validation precision at 5:  0.6902741205860671\n","Current best validation precision at 5:  0.6924189228726562\n","Average Loss for the current iteration:  851.908447265625\n","-----------------------------------\n","epoch:  6\n","iteration:  6 , batch:  0\n","iteration:  6 , batch:  300\n","iteration:  6 , batch:  600\n","iteration:  6 , batch:  900\n","iteration:  6 , batch:  1200\n","iteration:  6 , batch:  1500\n","iteration:  6 , batch:  1800\n","Train precision at 5:  0.8124614588256445\n","Validation precision at 5:  0.6916177165620317\n","Current best validation precision at 5:  0.6924189228726562\n","Average Loss for the current iteration:  835.9086303710938\n","-----------------------------------\n","epoch:  7\n","iteration:  7 , batch:  0\n","iteration:  7 , batch:  300\n","iteration:  7 , batch:  600\n","iteration:  7 , batch:  900\n","iteration:  7 , batch:  1200\n","iteration:  7 , batch:  1500\n","iteration:  7 , batch:  1800\n","Train precision at 5:  0.8202169558661293\n","Validation precision at 5:  0.6925775887291147\n","Current best validation precision at 5:  0.6924189228726562\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  825.2630004882812\n","-----------------------------------\n","epoch:  8\n","iteration:  8 , batch:  0\n","iteration:  8 , batch:  300\n","iteration:  8 , batch:  600\n","iteration:  8 , batch:  900\n","iteration:  8 , batch:  1200\n","iteration:  8 , batch:  1500\n","iteration:  8 , batch:  1800\n","Train precision at 5:  0.8277901559651542\n","Validation precision at 5:  0.6932313820809017\n","Current best validation precision at 5:  0.6925775887291147\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  817.4408569335938\n","-----------------------------------\n","epoch:  9\n","iteration:  9 , batch:  0\n","iteration:  9 , batch:  300\n","iteration:  9 , batch:  600\n","iteration:  9 , batch:  900\n","iteration:  9 , batch:  1200\n","iteration:  9 , batch:  1500\n","iteration:  9 , batch:  1800\n","Train precision at 5:  0.8281637521661778\n","Validation precision at 5:  0.6922017419485685\n","Current best validation precision at 5:  0.6932313820809017\n","Average Loss for the current iteration:  810.7973022460938\n","-----------------------------------\n","epoch:  10\n","iteration:  10 , batch:  0\n","iteration:  10 , batch:  300\n","iteration:  10 , batch:  600\n","iteration:  10 , batch:  900\n","iteration:  10 , batch:  1200\n","iteration:  10 , batch:  1500\n","iteration:  10 , batch:  1800\n","Train precision at 5:  0.8388765107014984\n","Validation precision at 5:  0.6935892242252547\n","Current best validation precision at 5:  0.6932313820809017\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  801.7613525390625\n","-----------------------------------\n","epoch:  11\n","iteration:  11 , batch:  0\n","iteration:  11 , batch:  300\n","iteration:  11 , batch:  600\n","iteration:  11 , batch:  900\n","iteration:  11 , batch:  1200\n","iteration:  11 , batch:  1500\n","iteration:  11 , batch:  1800\n","Train precision at 5:  0.8410832039250027\n","Validation precision at 5:  0.6927947696532027\n","Current best validation precision at 5:  0.6935892242252547\n","Average Loss for the current iteration:  788.3185424804688\n","-----------------------------------\n","epoch:  12\n","iteration:  12 , batch:  0\n","iteration:  12 , batch:  300\n","iteration:  12 , batch:  600\n","iteration:  12 , batch:  900\n","iteration:  12 , batch:  1200\n","iteration:  12 , batch:  1500\n","iteration:  12 , batch:  1800\n","Train precision at 5:  0.8450352215695449\n","Validation precision at 5:  0.6930659644858723\n","Current best validation precision at 5:  0.6935892242252547\n","Average Loss for the current iteration:  779.7584838867188\n","-----------------------------------\n","epoch:  13\n","iteration:  13 , batch:  0\n","iteration:  13 , batch:  300\n","iteration:  13 , batch:  600\n","iteration:  13 , batch:  900\n","iteration:  13 , batch:  1200\n","iteration:  13 , batch:  1500\n","iteration:  13 , batch:  1800\n","Train precision at 5:  0.849112146377683\n","Validation precision at 5:  0.6929129250782241\n","Current best validation precision at 5:  0.6935892242252547\n","Average Loss for the current iteration:  772.7719116210938\n","-----------------------------------\n","epoch:  14\n","iteration:  14 , batch:  0\n","iteration:  14 , batch:  300\n","iteration:  14 , batch:  600\n","iteration:  14 , batch:  900\n","iteration:  14 , batch:  1200\n","iteration:  14 , batch:  1500\n","iteration:  14 , batch:  1800\n","Train precision at 5:  0.8518769833231966\n","Validation precision at 5:  0.6930805932527785\n","Current best validation precision at 5:  0.6935892242252547\n","Average Loss for the current iteration:  767.4024658203125\n","-----------------------------------\n","epoch:  15\n","iteration:  15 , batch:  0\n","iteration:  15 , batch:  300\n","iteration:  15 , batch:  600\n","iteration:  15 , batch:  900\n","iteration:  15 , batch:  1200\n","iteration:  15 , batch:  1500\n","iteration:  15 , batch:  1800\n","Train precision at 5:  0.8508766007246762\n","Validation precision at 5:  0.6932437602682855\n","Current best validation precision at 5:  0.6935892242252547\n","Average Loss for the current iteration:  761.8590087890625\n","-----------------------------------\n","epoch:  16\n","iteration:  16 , batch:  0\n","iteration:  16 , batch:  300\n","iteration:  16 , batch:  600\n","iteration:  16 , batch:  900\n","iteration:  16 , batch:  1200\n","iteration:  16 , batch:  1500\n","iteration:  16 , batch:  1800\n","Train precision at 5:  0.854358247248656\n","Validation precision at 5:  0.6939706974546115\n","Current best validation precision at 5:  0.6935892242252547\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  756.5057983398438\n","-----------------------------------\n","epoch:  17\n","iteration:  17 , batch:  0\n"],"name":"stdout"},{"output_type":"stream","text":["iteration:  17 , batch:  300\n","iteration:  17 , batch:  600\n","iteration:  17 , batch:  900\n","iteration:  17 , batch:  1200\n","iteration:  17 , batch:  1500\n","iteration:  17 , batch:  1800\n","Train precision at 5:  0.8540071568428775\n","Validation precision at 5:  0.6935498390835814\n","Current best validation precision at 5:  0.6939706974546115\n","Average Loss for the current iteration:  751.7276611328125\n","-----------------------------------\n","epoch:  18\n","iteration:  18 , batch:  0\n","iteration:  18 , batch:  300\n","iteration:  18 , batch:  600\n","iteration:  18 , batch:  900\n","iteration:  18 , batch:  1200\n","iteration:  18 , batch:  1500\n","iteration:  18 , batch:  1800\n","Train precision at 5:  0.8545304165822619\n","Validation precision at 5:  0.6914095379560402\n","Current best validation precision at 5:  0.6939706974546115\n","Average Loss for the current iteration:  747.0913696289062\n","-----------------------------------\n","epoch:  19\n","iteration:  19 , batch:  0\n","iteration:  19 , batch:  300\n","iteration:  19 , batch:  600\n","iteration:  19 , batch:  900\n","iteration:  19 , batch:  1200\n","iteration:  19 , batch:  1500\n","iteration:  19 , batch:  1800\n","Train precision at 5:  0.85789503297098\n","Validation precision at 5:  0.6912339927531507\n","Current best validation precision at 5:  0.6939706974546115\n","Average Loss for the current iteration:  744.8367309570312\n","-----------------------------------\n","epoch:  20\n","iteration:  20 , batch:  0\n","iteration:  20 , batch:  300\n","iteration:  20 , batch:  600\n","iteration:  20 , batch:  900\n","iteration:  20 , batch:  1200\n","iteration:  20 , batch:  1500\n","iteration:  20 , batch:  1800\n","Train precision at 5:  0.8642495442576364\n","Validation precision at 5:  0.6943893052461162\n","Current best validation precision at 5:  0.6939706974546115\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  737.8028564453125\n","-----------------------------------\n","epoch:  21\n","iteration:  21 , batch:  0\n","iteration:  21 , batch:  300\n","iteration:  21 , batch:  600\n","iteration:  21 , batch:  900\n","iteration:  21 , batch:  1200\n","iteration:  21 , batch:  1500\n","iteration:  21 , batch:  1800\n","Train precision at 5:  0.8652791843899704\n","Validation precision at 5:  0.6934665676411832\n","Current best validation precision at 5:  0.6943893052461162\n","Average Loss for the current iteration:  727.0294189453125\n","-----------------------------------\n","epoch:  22\n","iteration:  22 , batch:  0\n","iteration:  22 , batch:  300\n","iteration:  22 , batch:  600\n","iteration:  22 , batch:  900\n","iteration:  22 , batch:  1200\n","iteration:  22 , batch:  1500\n","iteration:  22 , batch:  1800\n","Train precision at 5:  0.867941619967132\n","Validation precision at 5:  0.6938322868138708\n","Current best validation precision at 5:  0.6943893052461162\n","Average Loss for the current iteration:  722.1762084960938\n","-----------------------------------\n","epoch:  23\n","iteration:  23 , batch:  0\n","iteration:  23 , batch:  300\n","iteration:  23 , batch:  600\n","iteration:  23 , batch:  900\n","iteration:  23 , batch:  1200\n","iteration:  23 , batch:  1500\n","iteration:  23 , batch:  1800\n","Train precision at 5:  0.8674014808813175\n","Validation precision at 5:  0.6939819503522316\n","Current best validation precision at 5:  0.6943893052461162\n","Average Loss for the current iteration:  718.5333862304688\n","-----------------------------------\n","epoch:  24\n","iteration:  24 , batch:  0\n","iteration:  24 , batch:  300\n","iteration:  24 , batch:  600\n","iteration:  24 , batch:  900\n","iteration:  24 , batch:  1200\n","iteration:  24 , batch:  1500\n","iteration:  24 , batch:  1800\n","Train precision at 5:  0.8693977449193074\n","Validation precision at 5:  0.6941147345441607\n","Current best validation precision at 5:  0.6943893052461162\n","Average Loss for the current iteration:  714.5596313476562\n","-----------------------------------\n","epoch:  25\n","iteration:  25 , batch:  0\n","iteration:  25 , batch:  300\n","iteration:  25 , batch:  600\n","iteration:  25 , batch:  900\n","iteration:  25 , batch:  1200\n","iteration:  25 , batch:  1500\n","iteration:  25 , batch:  1800\n","Train precision at 5:  0.8697094501834125\n","Validation precision at 5:  0.6945153376994735\n","Current best validation precision at 5:  0.6943893052461162\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  711.3399658203125\n","-----------------------------------\n","epoch:  26\n","iteration:  26 , batch:  0\n","iteration:  26 , batch:  300\n","iteration:  26 , batch:  600\n","iteration:  26 , batch:  900\n","iteration:  26 , batch:  1200\n","iteration:  26 , batch:  1500\n","iteration:  26 , batch:  1800\n","Train precision at 5:  0.8702529651385138\n","Validation precision at 5:  0.6937872752233866\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  708.023193359375\n","-----------------------------------\n","epoch:  27\n","iteration:  27 , batch:  0\n","iteration:  27 , batch:  300\n","iteration:  27 , batch:  600\n","iteration:  27 , batch:  900\n","iteration:  27 , batch:  1200\n","iteration:  27 , batch:  1500\n","iteration:  27 , batch:  1800\n","Train precision at 5:  0.8705545427947615\n","Validation precision at 5:  0.692749758062717\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  706.6286010742188\n","-----------------------------------\n","epoch:  28\n","iteration:  28 , batch:  0\n","iteration:  28 , batch:  300\n","iteration:  28 , batch:  600\n","iteration:  28 , batch:  900\n","iteration:  28 , batch:  1200\n","iteration:  28 , batch:  1500\n","iteration:  28 , batch:  1800\n","Train precision at 5:  0.8730200526635509\n","Validation precision at 5:  0.6934024261247426\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  701.4879150390625\n","-----------------------------------\n","epoch:  29\n","iteration:  29 , batch:  0\n","iteration:  29 , batch:  300\n","iteration:  29 , batch:  600\n","iteration:  29 , batch:  900\n","iteration:  29 , batch:  1200\n","iteration:  29 , batch:  1500\n","iteration:  29 , batch:  1800\n","Train precision at 5:  0.8717844845047495\n","Validation precision at 5:  0.6936207323385926\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  701.267333984375\n","-----------------------------------\n","epoch:  30\n","iteration:  30 , batch:  0\n","iteration:  30 , batch:  300\n","iteration:  30 , batch:  600\n","iteration:  30 , batch:  900\n","iteration:  30 , batch:  1200\n","iteration:  30 , batch:  1500\n","iteration:  30 , batch:  1800\n","Train precision at 5:  0.8726171989287146\n","Validation precision at 5:  0.6934508135845133\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  698.41357421875\n","-----------------------------------\n","epoch:  31\n","iteration:  31 , batch:  0\n","iteration:  31 , batch:  300\n","iteration:  31 , batch:  600\n","iteration:  31 , batch:  900\n","iteration:  31 , batch:  1200\n","iteration:  31 , batch:  1500\n","iteration:  31 , batch:  1800\n","Train precision at 5:  0.8727027209506354\n","Validation precision at 5:  0.6932246303423291\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  696.8862915039062\n","-----------------------------------\n","epoch:  32\n","iteration:  32 , batch:  0\n","iteration:  32 , batch:  300\n","iteration:  32 , batch:  600\n","iteration:  32 , batch:  900\n","iteration:  32 , batch:  1200\n","iteration:  32 , batch:  1500\n","iteration:  32 , batch:  1800\n","Train precision at 5:  0.8735703193572243\n","Validation precision at 5:  0.692743006324145\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  694.6558227539062\n","-----------------------------------\n","epoch:  33\n","iteration:  33 , batch:  0\n","iteration:  33 , batch:  300\n","iteration:  33 , batch:  600\n","iteration:  33 , batch:  900\n","iteration:  33 , batch:  1200\n","iteration:  33 , batch:  1500\n","iteration:  33 , batch:  1800\n","Train precision at 5:  0.8741723493799551\n","Validation precision at 5:  0.693536335606435\n","Current best validation precision at 5:  0.6945153376994735\n","Average Loss for the current iteration:  692.4951782226562\n","-----------------------------------\n","epoch:  34\n","iteration:  34 , batch:  0\n","iteration:  34 , batch:  300\n","iteration:  34 , batch:  600\n","iteration:  34 , batch:  900\n","iteration:  34 , batch:  1200\n","iteration:  34 , batch:  1500\n","iteration:  34 , batch:  1800\n","Train precision at 5:  0.8771566178290796\n","Validation precision at 5:  0.6945941079828217\n","Current best validation precision at 5:  0.6945153376994735\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  690.3129272460938\n","-----------------------------------\n","epoch:  35\n","iteration:  35 , batch:  0\n"],"name":"stdout"},{"output_type":"stream","text":["iteration:  35 , batch:  300\n","iteration:  35 , batch:  600\n","iteration:  35 , batch:  900\n","iteration:  35 , batch:  1200\n","iteration:  35 , batch:  1500\n","iteration:  35 , batch:  1800\n","Train precision at 5:  0.8761888686336622\n","Validation precision at 5:  0.6920723336259247\n","Current best validation precision at 5:  0.6945941079828217\n","Average Loss for the current iteration:  688.4375\n","-----------------------------------\n","epoch:  36\n","iteration:  36 , batch:  0\n","iteration:  36 , batch:  300\n","iteration:  36 , batch:  600\n","iteration:  36 , batch:  900\n","iteration:  36 , batch:  1200\n","iteration:  36 , batch:  1500\n","iteration:  36 , batch:  1800\n","Train precision at 5:  0.8746832309319547\n","Validation precision at 5:  0.6931908716494646\n","Current best validation precision at 5:  0.6945941079828217\n","Average Loss for the current iteration:  688.0029296875\n","-----------------------------------\n","epoch:  37\n","iteration:  37 , batch:  0\n","iteration:  37 , batch:  300\n","iteration:  37 , batch:  600\n","iteration:  37 , batch:  900\n","iteration:  37 , batch:  1200\n","iteration:  37 , batch:  1500\n","iteration:  37 , batch:  1800\n","Train precision at 5:  0.8777271397384725\n","Validation precision at 5:  0.6940292125222397\n","Current best validation precision at 5:  0.6945941079828217\n","Average Loss for the current iteration:  685.3522338867188\n","-----------------------------------\n","epoch:  38\n","iteration:  38 , batch:  0\n","iteration:  38 , batch:  300\n","iteration:  38 , batch:  600\n","iteration:  38 , batch:  900\n","iteration:  38 , batch:  1200\n","iteration:  38 , batch:  1500\n","iteration:  38 , batch:  1800\n","Train precision at 5:  0.8778970584925513\n","Validation precision at 5:  0.6945085859609007\n","Current best validation precision at 5:  0.6945941079828217\n","Average Loss for the current iteration:  684.5603637695312\n","-----------------------------------\n","epoch:  39\n","iteration:  39 , batch:  0\n","iteration:  39 , batch:  300\n","iteration:  39 , batch:  600\n","iteration:  39 , batch:  900\n","iteration:  39 , batch:  1200\n","iteration:  39 , batch:  1500\n","iteration:  39 , batch:  1800\n","Train precision at 5:  0.8766626156235136\n","Validation precision at 5:  0.6933776697499761\n","Current best validation precision at 5:  0.6945941079828217\n","Average Loss for the current iteration:  682.9072875976562\n","-----------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lzNPRQ3yREGe","outputId":"56d2dfa7-81b2-4dda-ea77-dc12f88feeb4"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(val_p, color='b', label='Validation')\n","plt.plot(train_p, color='r', label='Training')\n","plt.vlines(np.argmax(val_p),0.67,1.0, linestyles='dashed', label='Best model')\n","plt.ylabel('Precision@5')\n","plt.xlabel('Epochs')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxyElEQVR4nO3deXgV5dn48e9NQha2sIUlBCRYFkEkQEQRQUARVNSiqGCrUvsryOvKa7XSWlxotVVrra0btihVX8FiS7HiCigWsBAgWECQpVgDCGFL2EIW7t8fz5zkEE5WcjInyf25rueafc59Jjlzzzwz84yoKsYYY0xJDfwOwBhjTGSyBGGMMSYkSxDGGGNCsgRhjDEmJEsQxhhjQor2O4Dq0rp1a+3cubPfYRhjTK2yatWqvaqaGGpanUkQnTt3Jj093e8wjDGmVhGRr0ubZlVMxhhjQrIEYYwxJiRLEMYYY0KqM9cgjDG1T35+PpmZmeTm5vodSp0XFxdHcnIyDRs2rPAyliCMMb7JzMykadOmdO7cGRHxO5w6S1XZt28fmZmZpKSkVHi5sFUxichMEdkjIutKmS4i8qyIbBGRL0SkX9C0W0Rks1duCVeMxhh/5ebm0qpVK0sOYSYitGrVqtJnauG8BvEqMKqM6ZcBXb0yEXgBQERaAg8B5wEDgIdEpEUY4zTG+MiSQ82oynYOW4JQ1SXA/jJmuRr4szqfA81FpD0wEvhIVfer6gHgI8pONMYYY8LAz7uYOgDfBA1neuNKG38KEZkoIukikp6VlRW2QI0xddOwYcP44IMPThr3zDPPMHny5JDzDx06tOiB3Msvv5yDBw+eMs/DDz/MU089Vebnzps3jw0bNhQNT5s2jY8//riS0Ydfrb7NVVVnqGqaqqYlJoZ8UtwYY0o1fvx4Zs+efdK42bNnM378+JPGHcrN51Bu/knjFixYQPPmzav0uSUTxKOPPsoll1xSpXWFk58JYgfQMWg42RtX2nhjjKlWY8eO5d133yUvLw+A7du3s3PnTt58803S0tLo1asXDz30EHsOHWfPoeMnLdu5c2f27t0LwC9/+Uu6devGhRdeyKZNm4rmefnllzn33HPp06cP1157LUePHmXZsmXMnz+f++67j9TUVLZu3cqECROYO3cuAAsXLqRv37707t2bW2+9lePHjxd93kMPPUS/fv3o3bs3GzduDPv28fM21/nAHSIyG3dBOltVd4nIB8BjQRemLwWm+hWkMaZm3HMPZGRU7zpTU+GZZ0qf3rJlSwYMGMB7773H1VdfzezZs7n++uv56U9/SsuWLSksLOTiiy9mwLDL6NHr7JDrWLVqFbNnzyYjI4OCggL69etH//79Abjmmmv40Y9+BMCDDz7In/70J+68806uuuoqRo8ezdixY09aV25uLhMmTGDhwoV069aNm2++mRdeeIF77rkHgNatW7N69Wqef/55nnrqKf74xz+e7iYqUzhvc30TWA50F5FMEfmhiNwmIrd5sywAtgFbgJeB/wFQ1f3AdGClVx71xhljTLULrmYKVC+99dZb9OvXj759+7J+/Xq2fFX60fpnn33GmDFjaNSoEc2aNeOqq64qmrZu3ToGDx5M7969eeONN1i/fn2ZsWzatImUlBS6desGwC233MKSJUuKpl9zzTUA9O/fn+3bt1f1K1dY2M4gVHV8OdMVuL2UaTOBmeGIyxgTmco60g+nq6++milTprB69WqOHj1Ky5Yteeqpp1i5ciUtWrRgwoQJHD9etSe9J0yYwLx58+jTpw+vvvoqn3zyyWnFGhsbC0BUVBQFBQWnta6KqNUXqY0x5nQ1adKEYcOGceuttzJ+/HhycnJo3LgxCQkJ7N69m/fee6/M5YcMGcK8efM4duwYhw4d4p133imadujQIdq3b09+fj5vvPFG0fimTZty6NChU9bVvXt3tm/fzpYtWwB47bXXuOiii6rpm1aeJQhjTL03fvx41q5dy/jx4+nTpw99+/alR48e3HjjjQwaNIgWjWLo0Dw+5LL9+vXjhhtuoE+fPlx22WWce+65RdOmT5/Oeeedx6BBg+jRo0fR+HHjxvHkk0/St29ftm7dWjQ+Li6OV155heuuu47evXvToEEDbrvtNvwirqan9ktLS1N7YZAxtcuXX37JWWed5XcY9Uao7S0iq1Q1LdT8dgZhjDHlyDmWT86x/PJnrGOsNVdjjClH1mH3LEKz+Io3lV0X2BmEMcaYkCxBGGOMCckShDHGmJAsQRhjjAnJEoQxpt7at28fqamppKam0q5dOzp06FA0HGjAD6Bji0Z0bNHopGXT09O56667yv2MCy64oNrjril2F5Mxpt5q1aoVGV4LgQ8//DBNmjThxz/+cdH0goICoqOjiYk+9Vg6LS2NtLSQjw+cZNmyZdUWb02zMwhjjAkyYcIEbrvtNs477zzuv/9+VqxYwYDzzuecPqlccMEFRc15f/LJJ4wePRpwyeXWW29l6NChdOnShWeffbZofU2aNCmaf+jQoYwdO5YePXrwve99j8CDygsWLKBHjx7079+fu+66q2i9frMzCGNMZPCjve9SZGZmsmzZMqKiosjJyeG1ee8THR3Nf9Z+zk9/+lPefvvtU5bZuHEjixcv5tChQ3Tv3p3JkyfTsOHJz02sWbOG9evXk5SUxKBBg1i6dClpaWlMmjSJJUuWkJKScsrLivxkCcIYY0q47rrriIqKAiA7O5s7J/0P2/+zldjoKPLzQz9RfcUVVxAbG0tsbCxt2rRh9+7dJCcnnzTPgAEDisalpqayfft2mjRpQpcuXUhJSQFcu1AzZswI47erOEsQxpjI4Fd73yE0bty4qP/nP/855184hBdmvUnUkb0MHTo05DKBprih9Oa4KzJPJLFrEMYYU4bs7GzatksC4NVXX6329Xfv3p1t27YVvQBozpw51f4ZVWUJwhhjynD//ffz1C8f4srhg8JyxB8fH8/zzz/PqFGj6N+/P02bNiUhIaHaP6cqrLlvY4xvaktz3wWFJwCIjgrPMfXhw4dp0qQJqsrtt99O165dmTJlSrV/jjX3bYwx1Sw6qkHYkgPAyy+/TGpqKr169SI7O5tJkyaF7bMqwy5SG2NMOfYfcU9Vt2wcE5b1T5kyJSxnDKfLziCMMaYcB47mceBoXvkz1jFhTRAiMkpENonIFhF5IMT0M0RkoYh8ISKfiEhy0LRCEcnwyvxwxmmMMeZUYatiEpEo4DlgBJAJrBSR+aq6IWi2p4A/q+osERkOPA7c5E07pqqp4YrPGGNM2cJ5BjEA2KKq21Q1D5gNXF1inp7AIq9/cYjpxhhjfBLOBNEB+CZoONMbF2wtcI3XPwZoKiKtvOE4EUkXkc9F5LuhPkBEJnrzpGdlZVVj6MaY+iIqKorU1FT69OlDv379qtz66jPPPMPRo0erObpTTZgwgblz5572PBXh90XqHwMXicga4CJgB1DoTTvDuzf3RuAZETmz5MKqOkNV01Q1LTExscaCNsbUHfHx8WRkZLB27Voef/xxpk6deso8Ka0ak9KqcYili9VUgqhJ4UwQO4COQcPJ3rgiqrpTVa9R1b7Az7xxB73uDq+7DfgE6BvGWI0xhpycHFq0aFE0/OSTT3LuueeSmtqHRx55GIAjR45wxRVX0KdPH84++2zmzJnDs88+y86dOxk2bBjDhg07Zb2dO3dm6tSppKamkpaWxurVqxk5ciRnnnkmL774IgCqyn333cfZZ59N7969i5rcUFXuuOMOunfvziWXXMKePXuK1rtq1Souuugi+vfvz8iRI9m1a1e1bo9wPgexEugqIim4xDAOdzZQRERaA/tV9QQwFZjpjW8BHFXV4948g4AnwhirMSYC3PDS8lPGjT6nPTcN7MyxvEImvLLilOlj+ydzXVpH9h/JY/Lrq06aNmfSwHI/89ixY6SmppKbm8uuXbtYtMhdFv3www/ZvHkzK1asYO+hXL53/bUsWbKErKwskpKSePfddwHXVlNCQgJPP/00ixcvpnXr1iE/p1OnTmRkZDBlyhQmTJjA0qVLyc3N5eyzz+a2227jr3/9a9GZzN69ezn33HMZMmQIy5cvZ9OmTWzYsIHdu3fTs2dPbr31VvLz87nzzjv5+9//TmJiInPmzOFnP/sZM2fOLPc7V1TYEoSqFojIHcAHQBQwU1XXi8ijQLqqzgeGAo+LiAJLgNu9xc8CXhKRE7iznF+VuPvJGGOqRaCKCWD58uXcfPPNrFu3jg8//JAPP/yQvn37kldwgiNHjrB582YGDx7Mvffey09+8hNGjx7N4MGDK/Q5V111FQC9e/fm8OHDNG3alKZNmxIbG8vBgwf55z//yfjx44mKiqJt27ZcdNFFrFy5kiVLlhSNT0pKYvjw4QBs2rSJdevWMWLECAAKCwtp3759tW6bsD5JraoLgAUlxk0L6p8LnHIlRVWXAb3DGZsxJvKUdcQfHxNV5vSWjWMqdMZQloEDB7J3716ysrJQVaZOncqkSZPYmnUYgDMT3dvhVq9ezYIFC3jwwQe5+OKLmTZtWlmrBYqb+m7QoMFJzX43aNCgSo0Aqiq9evVi+fJTz7qqi98XqY0xJmJs3LiRwsJCWrVqxciRI5k5cyaHD7vk8O2unezZs4edO3fSqFEjvv/973PfffexevVqAJo2bcqhQ4eq/NmDBw9mzpw5FBYWkpWVxZIlSxgwYABDhgwpGr9r1y4WL14MuGbCs7KyihJEfn4+69evP80tcDJri8kYU68FrkGAOyqfNWsWUVFRXHrppXz55ZcMHDiQvMITNGrUmLlz3mTLli3cd999NGjQgIYNG/LCCy8AMHHiREaNGkVSUlLRTrwyxowZw/Lly+nTpw8iwhNPPEG7du0YM2YMixYtomfPnnTq1ImBA91ZUkxMDHPnzuWuu+4iOzubgoIC7rnnHnr16lVt28aa+zbG+Ka2NPddsoqptqpsc992BmGMMeWo7YmhquwahDHGmJAsQRhjfFUbqrmzDuWSdSjX7zBOS1W2syUIY4xv4uLi2LdvX8QniZzcAnJyq/991DVFVdm3bx9xcXGVWs6uQRhjfJOcnExmZiaR3thm1qHjAOTtjS1nzsgVFxdHcnJy+TMGsQRhjPFNw4YNSUlJ8TuMcj3sNQEyZ1Kqv4HUMKtiMsYYE5KdQRhjTDniGkb5HYIvLEEYY0w5Zt06wO8QfGFVTMYYY0KyBGGMMeV4duFmnl242e8wapwlCGOMKcfSLXtZumWv32HUOEsQxhhjQrIEYYwxJiRLEMYYY0Ky21yNMaYcLRrF+B2CLyxBGGNMOV68qb/fIfjCqpiMMcaEFNYEISKjRGSTiGwRkQdCTD9DRBaKyBci8omIJAdNu0VENnvllnDGaYwxZfn1+xv59fsb/Q6jxoWtiklEooDngBFAJrBSROar6oag2Z4C/qyqs0RkOPA4cJOItAQeAtIABVZ5yx4IV7zGGFOa1V/Xz11POM8gBgBbVHWbquYBs4GrS8zTE1jk9S8Omj4S+EhV93tJ4SNgVBhjNcYYU0I4E0QH4Jug4UxvXLC1wDVe/xigqYi0quCyiMhEEUkXkfRIf+GIMcbUNn5fpP4xcJGIrAEuAnYAhRVdWFVnqGqaqqYlJiaGK0ZjjKmXwnmb6w6gY9BwsjeuiKruxDuDEJEmwLWqelBEdgBDSyz7SRhjNcaYUrVPqNy7nOuKcCaIlUBXEUnBJYZxwI3BM4hIa2C/qp4ApgIzvUkfAI+JSAtv+FJvujHG1LhnxvX1OwRfhK2KSVULgDtwO/svgbdUdb2IPCoiV3mzDQU2ichXQFvgl96y+4HpuCSzEnjUG2eMMaaGiKr6HUO1SEtL0/T0dL/DMMbUQY+8sx6Ah67s5XMk1U9EVqlqWqhp1tSGMcaUY8POHL9D8IXfdzEZY4yJUJYgjDHGhGQJwhhjTEh2DcIYY8rRJbGx3yH4otIJwnugrRuwTVUPVntExhgTYR6/5hy/Q/BFuQlCRJ5X1f/x+i8E/g/YCnxHRCap6oIwx2iMMXXX3r3wxReQlwcFBVBY6Epwf3Q0tGtXXBISQCTsoVXkDOL8oP7pwHdVdbWIdAHeAixBGGPqFlXYuhWWLYOlS/n2/UXEH84moUM7aNWquLRuXdyflAQdO7rSuJQqKVXYtg3++c/isrEK75mIiytOFu3bQ9++8POfn953DqGyVUzNVHU1gKpuExG7yG2Mqb0KCuDIETh8GP77X1i61CWFZctg9243T0ICu5LPIrtDN4YmRsO+fbB+vevu3++O8Etq1colik6dXLdNG/j3v11C+PZbN0/z5jBoENxyC6SluaQSHQ1RUcUlMJyX5+L59lvYtct1A2XzZpd4wqAiCaKHiHwBCNBZRFqo6gEvOdTPN3kbYyruxAl31JyRAXv2QNu2J1eXlHa0HVg2JwcOHnQlL8+NU3Xd4P7CQsjOhgMHQpeDB10iCJQjRyA399TPPPNMGDkSLrjA7cB79uRXL/8LgKGTBoaOb+9e2LnTJZlvvinu/uc/sGSJ++zOneGSS+DCC4vWS4NKHGP3qvmnuCuSIM4qMXzE67YEplVvOMaYWu34cXd0nZEBa9a47tq1cOhQ6cs0bVqcLKKj3c40sEPPzq760XFUlDtKb9HClebNoUMHaNIkdGnTBs47z8VRUQ0auPU2bw7f+U7p8+XmumqhWqbcBKGqX5cyfi/w12qPyBgTHgcPwqZNrmokL6+4HD9+8vDRo64cOeJKyf7geUuWo0fdUTW4nW6fPq4KJTXVlaQkdxYRqB4Jri7Ztcuto0MHOPvs4h1vYCefkACxsW6n3KCBu0gb3B8VBc2aFSeEpk1r5EJuhdTC5AAVvAYhIg2Bu4HLAqOAz4DpXqutxphwO3TIHZVv3ux2lI0bQ6NGJ3cbN4Zjx1wiCJSNG113z56Kf1Zg/cGlUSO3442Lg5gYVxo2LO6PiXHz9e7tksGZZ4auQmnf3iWOWqRnUjO/Q/BFRW5zjQP+AbwBXKqqhd74m4FpIvIXYLOqhqjMM8ZUybFjrnomPR1WrnTdjRsrX93SujV07w5XXum63bu7awCxscU79eD+hg0hPt4djZsidbEV14qoyBnE/cAcVX1FRP7o3d4K7iwCYAVwLfBwGOIzpu7573/dLZR797qSlVXcv3evq2r58sviu2PatYNzz4Vx49zdLmedVXz3TXAVUKDbsCF06+aSQatW/n5XU6tVJEFcAQzy+vcD7wLvAaOAgcD7uOTwcPWHZ0wdsnYt/OIX8Pbbp54JJCRAYqI74k9Jge9+1yWDtDRXJ298dc/sNUD9e7NcRRJEfNB1huGqej+AiPwdeFBVfyIidj5qTGnS02H6dJg/311EnTrV3e4YSAitWrmjfhOxdmXXzxr0iiSIDSIyQFVXAH/3rjl8BFwCvCMi3YHtYYzRmNpp2TKXGN5/313cfeQRuOsud1eOMbVARRLEr4Dfi8ilqjpdRM4BegC/ADbjLmBPDWOMxtQeOTnwySfwzDOweLE7S/jVr2DyZHf2YEwtUpHnIDJE5ElgiYi8CHwOfIFro+ll4Dfe2YUx9c+xY+5MYdEiV1audBeX27eHp5+GiRPLflLYmAhWoecgVHW+iHwKXA9M9kavAy5T1f3hCs6YsDh82D1PoFp8i2ds7MklOrr4gbHgB8cC/Vu2uISwbJl70Cwqyj2FO3UqDBvmmlKIjfX7m5pq0u+MFn6H4AvRMDXyBCAio4DfAVHAH1X1VyWmdwJmAc29eR5Q1QUi0hn4Etjkzfq5qt5W1melpaVpenp69X4BUzcUFLgLxR9/DB99BMuXQ37+6a1TxLWgOXy4Kxde6J7cNaaWEZFVqpoWalqFW3MVkUG4W1nPCF5OVbuUMn8U8BwwAsgEVorIfFXdEDTbg8BbqvqCiPTENR3e2Zu2VVVTKxqfqQf27XNVOP/6l+sePeruAgrcCRTob93a1fevWeMSwuLFrk0fcDv1KVNgyBD3RPDx48Ul0OzE8eMuqQQ/pVyyv21bd+HZmDqsMs19/wmYAqwCQrRve4oBwBZV3QYgIrOBq4HgBKFA4MpdArCzEvGYuiwvD1atghUrXPnXv9zDZeCO3nv2dHcDffGFSxz79oV+yviMM+C662DECFf1k5hYo1/D1A23vbYKgBdv6u9zJDWrMgkiW1Xfq8T8HYBvgoYzgfNKzPMw8KGI3Ak0xt06G5AiImuAHNzzFp+V/AARmQhMBOjUqVMlQjMRKScHFiyAefNcN9ACaHIyDBjgLvgOGAD9+59anXPihGuMLvA08oED0KMHdOkSOQ22mVrrwNE8v0PwRWUSxGLvbqa/AscDIwMvEKqi8cCrqvobERkIvCYiZwO7gE6quk9E+gPzRKSXquYEL6yqM4AZ4K5BnEYcxi+7drkHyObNg4UL3bWBNm3ghhvg8svdhd+kpPLX06ABtGzpSrduYQ/bmPqgMgkicPQffDFDgeGlzL8D6Bg0nOyNC/ZDXJMdqOpyr2HA1qq6By8JqeoqEdkKdAPsKnRtpwobNsA778Df/w6ff+7Gn3km3H23a2Li/POtsThjIkCFE4SqDqvkulcCXUUkBZcYxgE3lpjnv8DFwKsichYQB2SJSCKwX1ULvcYBuwLbKvn5JlLk58Nnn7kzhXfecW8XA1dVNH06jBnjrilYVZAxEaUydzElAA8BQ7xRnwKPqmp2qPlVtUBE7gA+wN3COlNV14vIo0C6qs4H7gVeFpEpuLORCaqqIjIEeFRE8oETwG32vEUtUlDgnhNYtQrefdddT8jOds8FXHIJ3H8/jB5tjdCZWmPQd1r7HYIvKvwchIi8jXs4bpY36iagj6peE6bYKsWeg/BBYaF75+769a6sW+e6Gze6u5DAXU8YPdq9j2DECHuq2JgIUy3PQQBnquq1QcOPiEjGaUVmaq8lS9yF5G+/LR53xhnuxeojR7pXRvbu7d4cVpkXsxtjIkZlEsQxEblQVf8JRQ/OHQtPWCaizZgBt9/uLiz/4hcuGfTsaU8SmzrrlpmuublZtw7wOZKaVZkEMRmY5V2LENzLgyaEIygTofLz4X//F/7wBxg1CmbPdi+6MaaOy82vyLPBdU9l7mLKAPqISDNvOKfsJUydsm8fXH+9a6Duxz92TVjbrajG1GnlJggR+b6qvi4i/1tiPACq+nSYYjORYsMGuOoq+OYbePVVuOUWvyMyxtSAipxBBG47sQrm+ugf/4Abb3QN1H36qXuIzRhTL1TkhUEved1Hwh+OqVFbt7r3GeTmuhZMA91Af1YWzJoF/fq5pjCSk/2O2BhfXHxWG79D8EVlHpR7Avea0WPA+8A5wBRVfT1MsZlwyclxTzD/7nelvxchLs492HbzzfD88+4Mwph6auKQM/0OwReVuYvpUlW9X0TGANuBa4AlgCWI2uLECXdGMHUq7N4NP/gB3HuvuxMpNrY4KTRsaM1eGGMqlSAC814B/EVVs8V2IrXH55/DXXe5F+0MHOjaRDr3XL+jMqZWuOGl5QDMmTTQ50hqVmUecf2HiGwE+gMLvQb1csMTlqk2O3fCTTe5pLBjB7z2GixdasnBGFOuCicIVX0AuABIU9V84AjuDXEmEuXlwa9/7d6N8NZbrlpp0yb4/vet+sgYUyEVeQ5iuKouEpFrgsYFz/LXcARmTsOHH8Kdd8JXX7nnF55+2jWLYYwxlVCRaxAXAYuAK0NMUyxBRI7t211TGH/7G3Tt6prZvuwyv6MyxtRSFXkO4iGv+4Pwh2OqJDcXnnwSHnvMtZz62GMuUcTG+h2ZMXXC6HPa+x2CLyrzHMRjwBOqetAbbgHcq6oPhik2Ux5V90Keu+92b2m7/np46ino2LH8ZY0xFXbTwM5+h+CLytzFdFkgOQCo6gHg8mqPyFTM0qUwbJh7EU9cHCxcCHPmWHIwJgyO5RVyLK/+tehamQQRJSJFdRYiEg9YHUZNW7MGrrgCLrzQvbnt97+HjAwYPtzvyIypsya8soIJr6zwO4waV5kH5d7APf/wijf8A4pfP2rCbeNGmDYN/vIXaNHCNbd9xx32Ck9jTNhU5n0QvxaRtcAl3qjpqvpBeMIyRb7+Gh55xDWRER8PP/+5uwDdvLnfkRlj6rjKnEEAfAkUqOrHItJIRJqq6qFwBGZwVUeDBkFhobsQ/cAD0KZ+tippjKl5Fb4GISI/AuYCL3mjOgDzyllmlIhsEpEtIvJAiOmdRGSxiKwRkS9E5PKgaVO95TaJyMiKxllnHDgA117rzhQ2bnQPu1lyMMbUoMqcQdwODAD+BaCqm0Wk1D2WiEQBzwEjgExgpYjMV9UNQbM9CLylqi+ISE9gAdDZ6x8H9AKSgI9FpJuq1o/bCE6ccE1ifPONe0lP585+R2RMvTa2f/18F0plEsRxVc0LNLMhItG4J6lLMwDYoqrbvPln49puCk4QCjTz+hOAnV7/1cBsVT0O/EdEtnjrW16JeGuv6dPdU9DPPeca2TPG+Oq6tPp5+3hlbnP9VER+CsSLyAjgL8A7ZczfAfgmaDjTGxfsYeD7IpKJO3u4sxLLIiITRSRdRNKzsrIq8VUi2IIF7qL0TTfB5Ml+R2OMAfYfyWP/kTy/w6hxlUkQPwGygH8Dk3A79NN9ino88KqqJuMeuntNRCrTwuwMVU1T1bTExMTTDCUCbNsG3/senHMOvPiitbpqTISY/PoqJr++yu8walyFqpi86wnrVbUH8HIF170DCD4vS/bGBfshMApAVZeLSBzQuoLL1i1Hj7qL0gBvv22v+DTG+K5CR+vexeFNItKpEuteCXQVkRQRicFddJ5fYp7/AhcDiMhZQBzuLGU+ME5EYkUkBegK1N3HGFVddVJGBrz+ujXNbYyJCJW5SN0CWC8iK3AvCwJAVa8KNbOqFojIHcAHQBQwU1XXi8ijQLqqzgfuBV4WkSm4C9YTVFW9z3kLd0G7ALi9Tt/B9NJL8Oc/w0MPuWY0jDEmAlQmQfy8sitX1QW4axXB46YF9W8ABpWy7C+BX1b2M2udpUvdu6Ivu8w1pWGMMRGiIm+UiwNuA76Du0D9J1UtCHdgdUpeHmzd6l75GSgbN7ru/v2QkuKqlhpU5p4BY0xN+f75Z/gdgi8qcgYxC8gHPgMuA3oCd4czqDojOxsmTYK5c11zGQHt2kH37jB2rOtedx20bOlfnMaYMl3ZJ8nvEHxRkQTRU1V7A4jIn6jLF4ur07p1cM017tbVO+6A/v1dMujeHRIS/I7OGFMJOw8eAyCpebzPkdSsiiSI/ECPd+E5jOHUEW+8ARMnQrNmsHgxDB7sd0TGmNMwZU4GAHMm1a+WDSqSIPqISI7XL7gnqXO8flXVZqUvWs/k5bmmuJ97DoYMcW94a9fO76iMMaZKyk0QqhpVE4HUepmZ7lrC55/DvffC449Dw4Z+R2WMMVVW2fdBmFAWLYJx4+DYMffGt7Fj/Y7IGGNOm91XebqeeQZGjIDEREhPt+RgjKkz7AyiqgoL3fWGZ591dyvNmgVNmvgdlTEmDH40uIvfIfjCEkRVHD3qXujzt7+5JPHkk/aQmzF12CU92/odgi8sQVRWVhZceSWsWAG/+51rJsMYU6dtzToMwJmJ9auWwBJEZXz1FVx+OezY4ZrkHjPG74iMMTXgp3/9N2DPQZjSLFsGV13lXuKzeDGcf77fERljTFhZxXlFvP02DB/u2ktavtySgzGmXrAEUZ733nMPwPXr584ivvMdvyMyxpgaYVVMZcnJca2xnnUWLFwI8fWroS5jTP1mCaIsU6e6JjSWLbPkYEw9dufwrn6H4AtLEKX57DN4/nm4+2675mBMPXdh19Z+h+ALuwYRSm4u/L//B507wy9+4Xc0xhifrd+Zzfqd2X6HUePsDCKU6dPdMw8ffmjNZxhjePSdDUD9ew7CziBKysiAX/8aJkxwjfAZY0w9FdYEISKjRGSTiGwRkQdCTP+tiGR45SsRORg0rTBo2vxwxlmkoAB++ENo3Rp+85sa+UhjjIlUYatiEpEo4DlgBJAJrBSR+aq6ITCPqk4Jmv9OoG/QKo6pamq44gvpt7+F1avdOx1atqzRjzbGmEgTzjOIAcAWVd2mqnnAbODqMuYfD7wZxnjKtnkzTJvm2le69lrfwjDGmEgRzovUHYBvgoYzgfNCzSgiZwApwKKg0XEikg4UAL9S1XkhlpsITATo1KlT1SM9cQJ+9COIjYU//MG1t2SMMZ77R3X3OwRfRMpdTOOAuapaGDTuDFXdISJdgEUi8m9V3Rq8kKrOAGYApKWlaZU//Y9/hE8/dd2kpCqvxhhTN/U/o35WOYezimkH0DFoONkbF8o4SlQvqeoOr7sN+ISTr09Unx074L77XGN8t94alo8wxtRuq77ez6qv9/sdRo0LZ4JYCXQVkRQRicElgVPuRhKRHkALYHnQuBYiEuv1twYGARtKLlstEhLcQ3EzZljVkjEmpCfe38QT72/yO4waF7YqJlUtEJE7gA+AKGCmqq4XkUeBdFUNJItxwGxVDa4iOgt4SURO4JLYr4LvfqpWTZrYLa3GGBNCWK9BqOoCYEGJcdNKDD8cYrllQO9wxmaMMaZs9iS1McaYkCxBGGOMCSlSbnM1xpiINe3Knn6H4AtLEMYYU45eSQl+h+ALq2Iyxphy/HPzXv65ea/fYdQ4O4Mwxphy/H7RZqD+vVnOziCMMcaEZAnCGGNMSJYgjDHGhGQJwhhjTEh2kdoYY8rx2DX1s+UfSxDGGFOOMxOb+B2CL6yKyRhjyvHxht18vGG332HUODuDMMaYcrz82TYALunZ1udIapadQRhjjAnJEoQxxpiQLEEYY4wJyRKEMcaYkOwitTHGlOO3N6T6HYIvLEEYY0w5kprH+x2CL6yKyRhjyvHO2p28s3an32HUuLAmCBEZJSKbRGSLiDwQYvpvRSTDK1+JyMGgabeIyGav3BLOOI0xpiyvf/41r3/+td9h1LiwVTGJSBTwHDACyARWish8Vd0QmEdVpwTNfyfQ1+tvCTwEpAEKrPKWPRCueI0xxpwsnGcQA4AtqrpNVfOA2cDVZcw/HnjT6x8JfKSq+72k8BEwKoyxGmOMKSGcCaID8E3QcKY37hQicgaQAiyqzLIiMlFE0kUkPSsrq1qCNsYY40TKRepxwFxVLazMQqo6Q1XTVDUtMTExTKEZY0z9FM7bXHcAHYOGk71xoYwDbi+x7NASy35SjbGZeuj4ccjJceXwYWjYEOLjIS6uuMTGQoMwHTapwpdfwrJlrr9ly+LSqpXrxseDyMnLnTgBeXmQm+u+Q14eFBaWXU6cCN0tLIT8fLeu4HL8eHF/XBw0a1ZcEhJO7m/Z0m27uujIEcjKgr17XTdQ2u8aQKNGyrvvQr9+0L796X2OKhw9CgcOwMGDkJ3tho8ehWPHTu0CJCZCmzbQtq3rtmnj/h4l/1+qUzgTxEqgq4ik4Hb444AbS84kIj2AFsDyoNEfAI+JSAtv+FJgahhjrXaFhbB7N+zY4f7AzZu7P2bgxxaOnVBBgfuH27evuBw4AIcOlV5EoHVr98/XuvWpJTcXMjPd99ix4+T+XbsgJsZ9p+bNi79joNu0qdsx5ee7nVpwNz/fxRsVVXoRcduxoKB451Zaf6hpR44UJ4ScHLcTrIjAd+reHXr2PLkkJVX8B1lYCGvXwpIlrnz2mdvxlCU21n12YaGL9/hxt61qiojbeZWnefOT/28C3YQEt91D/a8dPux2eAUFxX//QDfQD+5vHx1derdhQ9cN1R8f70qjRqd2GzZ0/weBHXJw9+BB91sJ7IxLio6OoqCgeLhtW+jb1yWLvn1diY6GPXtcycoq7g+UAweKE8LBg9Xzd42JcYli0CCYPfv011dS2BKEqhaIyB24nX0UMFNV14vIo0C6qs73Zh0HzFYt/rdU1f0iMh2XZAAeVdX94YgzNxeWLi0+agiU4OHsbGjcuHgHX7JERcHOncU7zsDOs7CUCjMRt/MM7EjbtIEOHdzOp0OH4pKUBO3auR/Xjh3FnxHc3bXLxbhvn4uzLFFR7nODy4kTsGWLW0dOTtnLR0e7I6cOHaB3b7j0UvfDDv6xffttcf/hw+67xsS4H2egBIajok4+si1ZVIt3DME7iYoMR0e7OM86K/RRcOPGpx5JHztW3L9/P2zcCHPnuv6AhASXKDp2LP4OoXZkX33l/q8C27RLFxg9GoYMgQsvdDut/ftd2bevuH//frf9oqNdsggugTOcmJiyE2ugNGgQuhsTc+pZU6A/Otptl+DEmpPj/p6Bneu+fSf/Tr75Blavdv15eSf/nzVpUtzftm3xjrq0HT2UfTAQnExKdvPzXWzHjp16FJ6XV/w/HDiYCfz+2rd33RYtXKILTnqB8sFX33DsiNAlKpk1a9z3XbMGPv6YkxJHSfHx7nsnJrozr5QU9znNmxd3AyVUUgskPFW3vXfvPjXx7N59+mc0pRGtyOFCLZCWlqbp6emVXi4ry+2gg7VsWXwEnZhYfFSUnX1qyc11yzRrVrxjT04+uT8+vnj+UEcvgTONXbvK/mcrGWNSkiutW7sqikA1RaC/VSv3T9ismfuBxsaWffSbl3fqjz82tvh7tGlTuTOfEyfCV11TU1TddtiwwZX16113586yd2IdOsBFF7mEMHiw2351nar7H4qJCW+1R1UEqtbK+w2U5oaXXAXHnEkDTxqfmwvr1kFGhltvoOonUBo3robgw0xEVqlqWqhp9b6pjZYt4dNPixNCy5bFRzIVEagyqY5/hBMn3M4o+Czh22/dDj5wdpGU5I4W4sPw5H9MjFt3dR2N1PbkACf/6IcO9TuayCbidsCRKHBWVd3i4iAtzZW6qN4niKgod5RXVTExrlSHBg3c6Wjbtq5u0xhj/FQHjvGMMcaEgyUIY4wxIdX7KiZjjCnPqz8Y4HcIvrAEYYwx5YiPCcMV7lrAqpiMMaYcry3fzmvLt/sdRo2zBGGMMeX4xxe7+McXu/wOo8ZZgjDGGBOSJQhjjDEhWYIwxhgTkiUIY4wxIdltrsYYU46SjfTVF3YGYYwxJiRLEMYYY0KyBGGMMSYkSxDGGGNCsgRhjDEmJEsQxhhjQrIEYYwxJiRLEMYYY0KyBGGMMSYkUVW/Y6gWIpIFfH0aq2gN7K2mcKqbxVY1FlvVWGxVU1tjO0NVE0NNqDMJ4nSJSLqqpvkdRygWW9VYbFVjsVVNXYzNqpiMMcaEZAnCGGNMSJYgis3wO4AyWGxVY7FVjcVWNXUuNrsGYYwxJiQ7gzDGGBOSJQhjjDEh1fsEISKjRGSTiGwRkQf8jieYiGwXkX+LSIaIpEdAPDNFZI+IrAsa11JEPhKRzV63RYTE9bCI7PC2XYaIXF7TcXlxdBSRxSKyQUTWi8jd3vhI2G6lxeb7thOROBFZISJrvdge8caniMi/vN/rHBGJiaDYXhWR/wRtt9Saji0oxigRWSMi//CGq7bdVLXeFiAK2Ap0AWKAtUBPv+MKim870NrvOILiGQL0A9YFjXsCeMDrfwD4dYTE9TDw4wjYZu2Bfl5/U+AroGeEbLfSYvN92wECNPH6GwL/As4H3gLGeeNfBCZHUGyvAmP9/p/z4vpf4P+Af3jDVdpu9f0MYgCwRVW3qWoeMBu42ueYIpaqLgH2lxh9NTDL658FfLcmY4JS44oIqrpLVVd7/YeAL4EORMZ2Ky0236lz2Bts6BUFhgNzvfF+bbfSYosIIpIMXAH80RsWqrjd6nuC6AB8EzScSYT8QDwKfCgiq0Rkot/BlKKtqu7y+r8F2voZTAl3iMgXXhVUjVfhlCQinYG+uCPOiNpuJWKDCNh2XjVJBrAH+Ah3tn9QVQu8WXz7vZaMTVUD2+2X3nb7rYjE+hEb8AxwP3DCG25FFbdbfU8Qke5CVe0HXAbcLiJD/A6oLOrOXyPlSOoF4EwgFdgF/MbPYESkCfA2cI+q5gRP83u7hYgtIradqhaqaiqQjDvb7+FHHKGUjE1Ezgam4mI8F2gJ/KSm4xKR0cAeVV1VHeur7wliB9AxaDjZGxcRVHWH190D/A33I4k0u0WkPYDX3eNzPACo6m7vR3wCeBkft52INMTtgN9Q1b96oyNiu4WKLZK2nRfPQWAxMBBoLiLR3iTff69BsY3yquxUVY8Dr+DPdhsEXCUi23FV5sOB31HF7VbfE8RKoKt3hT8GGAfM9zkmAESksYg0DfQDlwLryl7KF/OBW7z+W4C/+xhLkcDO1zMGn7adV//7J+BLVX06aJLv26202CJh24lIoog09/rjgRG4aySLgbHebH5tt1CxbQxK+IKr46/x7aaqU1U1WVU74/Zni1T1e1R1u/l9td3vAlyOu3tjK/Azv+MJiqsL7q6qtcD6SIgNeBNX5ZCPq8f8Ia5+cyGwGfgYaBkhcb0G/Bv4Arczbu/TNrsQV330BZDhlcsjZLuVFpvv2w44B1jjxbAOmOaN7wKsALYAfwFiIyi2Rd52Wwe8jnenk18FGErxXUxV2m7W1IYxxpiQ6nsVkzHGmFJYgjDGGBOSJQhjjDEhWYIwxhgTkiUIY4wxIVmCMKYcIlIY1EJnhlRjq78i0jm4FVpjIkl0+bMYU+8dU9esgjH1ip1BGFNF4t7X8YS4d3asEJHveOM7i8gir9G2hSLSyRvfVkT+5r1HYK2IXOCtKkpEXvbeLfCh93QuInKX966GL0Rktk9f09RjliCMKV98iSqmG4KmZatqb+APuFY0AX4PzFLVc4A3gGe98c8Cn6pqH9z7K9Z747sCz6lqL+AgcK03/gGgr7ee28Lz1YwpnT1JbUw5ROSwqjYJMX47MFxVt3mN3n2rqq1EZC+ueYp8b/wuVW0tIllAsrrG3ALr6IxrLrqrN/wToKGq/kJE3gcOA/OAeVr8DgJjaoSdQRhzerSU/so4HtRfSPG1wSuA53BnGyuDWuM0pkZYgjDm9NwQ1F3u9S/DtaQJ8D3gM69/ITAZil44k1DaSkWkAdBRVRfj3iuQAJxyFmNMONkRiTHli/feHhbwvqoGbnVtISJf4M4Cxnvj7gReEZH7gCzgB974u4EZIvJD3JnCZFwrtKFEAa97SUSAZ9W9e8CYGmPXIIypIu8aRJqq7vU7FmPCwaqYjDHGhGRnEMYYY0KyMwhjjDEhWYIwxhgTkiUIY4wxIVmCMMYYE5IlCGOMMSH9f6KvibO6xasSAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"x8XgKeH4REGf","outputId":"31fc9833-9360-4646-d9b5-c9ccb22bce12"},"source":["## n_users=14811\n","## n_beers=52583\n","## device:gpu\n","hidden_size=150\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","best_model = NeuMF(gmf_model, mlp_model, hidden_size, device)\n","torch.save(best_state_dict, \"../Models/checkpoints/neu_mf.pth\")\n","# best_model.load_state_dict(best_state_dict['model_state_dict'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["n_input 150\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2S7IbsOyREGf","outputId":"eeb89144-ffff-49ad-c3f1-671890483a53"},"source":["#label_pred = 'pred_y'\n","best_model.to(device)\n","best_model.eval()\n","test_data[label_pred] = (best_model.predict(test_data)>0.5).cpu().detach().numpy().astype(int)\n","test_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>beer_id</th>\n","      <th>review_overall</th>\n","      <th>pred_y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17</td>\n","      <td>5892</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>300822</th>\n","      <td>14731</td>\n","      <td>14334</td>\n","      <td>2.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>300823</th>\n","      <td>14776</td>\n","      <td>42943</td>\n","      <td>4.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>300824</th>\n","      <td>14780</td>\n","      <td>22042</td>\n","      <td>4.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>300825</th>\n","      <td>14785</td>\n","      <td>36467</td>\n","      <td>3.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>300826</th>\n","      <td>14790</td>\n","      <td>46338</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>300827 rows × 4 columns</p>\n","</div>"],"text/plain":["        user_id  beer_id  review_overall  pred_y\n","0             0     5892             4.0       1\n","1             2     5892             4.0       1\n","2             9     5892             4.0       1\n","3            12     5892             4.0       1\n","4            17     5892             5.0       1\n","...         ...      ...             ...     ...\n","300822    14731    14334             2.5       0\n","300823    14776    42943             4.5       0\n","300824    14780    22042             4.5       0\n","300825    14785    36467             3.5       0\n","300826    14790    46338             2.0       0\n","\n","[300827 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"j1k3JwePREGf","outputId":"e3e71ec1-d5d8-4117-8288-d5f1a1ca3592"},"source":["#threshold=4\n","#k=5\n","test_prec, test_recall = precision_recall_at_k(test_data, label_pred=label_pred, threshold=threshold, k=k)\n","# Precision and recall can then be averaged over all users\n","print(\"precision at 5 for test set: \", sum(prec for prec in test_prec.values()) / len(test_prec))\n","print(\"recall at 5 for test set:\", sum(rec for rec in test_recall.values()) / len(test_prec))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["precision at 5 for test set:  0.6406764394604831\n","recall at 5 for test set: 0.5465328089842406\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nro-UqKXREGg","outputId":"aad54ce4-b19d-449f-9e2a-c34eeb020e87"},"source":["best_model.eval()\n","val_data[label_pred] = (best_model.predict(val_data)>0.5).cpu().detach().numpy().astype(int)\n","val_prec, val_recall = precision_recall_at_k(val_data, label_pred=label_pred, threshold=threshold, k=k)\n","# Precision and recall can then be averaged over all users\n","print(\"precision at 5 for validation set: \", sum(prec for prec in val_prec.values()) / len(val_prec))\n","print(\"recall at 5 for validation set:\", sum(rec for rec in val_recall.values()) / len(val_prec))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["precision at 5 for validation set:  0.6554148943353072\n","recall at 5 for validation set: 0.5729507226299717\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yzm1gvajREGg","outputId":"1534196e-3741-4b33-fc52-2bb8761f79b0"},"source":["precisions, recalls = precision_recall_at_k(test_data[test_data['user_id']==0], label_pred=label_pred, threshold=threshold, k=k)\n","print(\"precision at 5 for user 0: \", sum(prec for prec in precisions.values()) / len(precisions))\n","print(\"recall at 5 for user 0:\", sum(rec for rec in recalls.values()) / len(recalls))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["precision at 5 for user 0:  0.4\n","recall at 5 for user 0: 0.0034602076124567475\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QD-7mGM2REGg"},"source":[""],"execution_count":null,"outputs":[]}]}