{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NeuMF.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOH6AjUF76BKiOAOoIhHJpv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKgktStsqxKc","executionInfo":{"status":"ok","timestamp":1622473227008,"user_tz":240,"elapsed":138,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"58c2128e-3b0d-4625-c07e-987093605383"},"source":["# Mount your google drive in google colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/CS247/Models')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EMFc_rrpq0ss","executionInfo":{"status":"ok","timestamp":1622473227535,"user_tz":240,"elapsed":400,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}}},"source":["import numpy as np\n","import pandas as pd\n","from util import *\n","from model import MLPEmbedding\n","from model import GMF\n","from evaluate import *\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"BS7SdOI3q2MZ","executionInfo":{"status":"ok","timestamp":1622473227971,"user_tz":240,"elapsed":438,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}}},"source":["train_data = pd.read_pickle(\"/content/drive/MyDrive/CS247/Data/train_data.pkl\")\n","val_data = pd.read_pickle(\"/content/drive/MyDrive/CS247/Data/val_data.pkl\")\n","test_data = pd.read_pickle(\"/content/drive/MyDrive/CS247/Data/test_data.pkl\")[['review_profilename','beer_name','review_overall']]\n","test_data = merge_user_id(test_data, on='review_profilename').rename(columns={'id':'user_id'})\n","test_data = merge_beer_id(test_data, on='beer_name').rename(columns={'id':'beer_id'})[['user_id','beer_id','review_overall']]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"SfHpw65IlJ_o","executionInfo":{"status":"ok","timestamp":1622473227974,"user_tz":240,"elapsed":10,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"0d3e72b9-2887-4afe-9c3e-9c21d3c9f78b"},"source":["# convert review score to boolean - if the item is relevant to the user (rating >= 3.5) then 1, else 0\n","threshold = 4.0\n","train_data['relevant'] = (train_data['review_overall'] >= threshold).astype(int)\n","train_data.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>beer_id</th>\n","      <th>review_overall</th>\n","      <th>relevant</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>383941</th>\n","      <td>415</td>\n","      <td>831</td>\n","      <td>3.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>483700</th>\n","      <td>2492</td>\n","      <td>3491</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>375376</th>\n","      <td>1233</td>\n","      <td>1178</td>\n","      <td>4.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>705138</th>\n","      <td>1230</td>\n","      <td>902</td>\n","      <td>4.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>435972</th>\n","      <td>2290</td>\n","      <td>7497</td>\n","      <td>3.5</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        user_id  beer_id  review_overall  relevant\n","383941      415      831             3.5         0\n","483700     2492     3491             4.0         1\n","375376     1233     1178             4.5         1\n","705138     1230      902             4.5         1\n","435972     2290     7497             3.5         0"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxzr2MIilLQ6","executionInfo":{"status":"ok","timestamp":1622473227974,"user_tz":240,"elapsed":9,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"d459529b-404d-4331-f0a4-17e3a629220d"},"source":["### unique users & beers in train_data\n","n_users = len(train_data['user_id'].unique())\n","n_beers = len(train_data['beer_id'].unique())\n","print(\"unique users: \", n_users)\n","print(\"unique beers: \", n_beers)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["unique users:  14811\n","unique beers:  52583\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xpqp9HK6q58l","executionInfo":{"status":"ok","timestamp":1622473228480,"user_tz":240,"elapsed":4,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}}},"source":["class NeuMF(nn.Module):\n","  def __init__(self, gmf_model, mlp_model, n_input, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n","    ## n_input = sum of output shape of the last layer of gmf and mlp model\n","    super().__init__()\n","    self.gmf_model=gmf_model\n","    self.mlp_model=mlp_model\n","    self.net = nn.Linear(n_input, 1) ## we can add more layers here\n","    self.device = device\n","  \n","  def forward(self, df):\n","    # df: user_id, beer_id\n","    mlp_out = self.mlp_model.forward(df) # (n,25)\n","    ## TODO: output of the gmf_model\n","    gmf_out = self.gmf_model.forward_no_h(df)\n","    input = torch.cat((mlp_out, gmf_out), axis=1).to(self.device)\n","    return self.net(input)\n","\n","  def predict(self, df):\n","    return torch.sigmoid(self.forward(df))\n","\n","  def loss(self, df, loss_fn):\n","    y_pred = self.forward(df).view(-1)\n","    y_train = torch.Tensor(df.relevant.to_numpy()).to(self.device)\n","    \n","    return loss_fn(y_pred, y_train)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYL4VnXTzUCl","executionInfo":{"status":"ok","timestamp":1622473228480,"user_tz":240,"elapsed":3,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","n_hidden = 150\n","epochs = 40\n","batch_size = 500\n","lr = 0.01\n","loss_fn = nn.BCEWithLogitsLoss()\n","label_pred = 'pred_y'\n","k=5"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5xsGR6WyvfM","executionInfo":{"status":"ok","timestamp":1622473230735,"user_tz":240,"elapsed":2258,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"ee8c718a-9b41-439b-f40e-c05133d64165"},"source":["# https://discuss.pytorch.org/t/merging-two-models/45637\n","gmf_model =  GMF(n_users,n_beers)\n","gmf_model.load_state_dict(torch.load(\"/content/drive/MyDrive/CS247/Models/checkpoints/gmf.pth\"))\n","gmf_model.to(device)\n","mlp_model = MLPEmbedding(n_users=n_users, n_beers=n_beers, device=device, hidden_size=150).to(device)\n","mlp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/CS247/Models/checkpoints/mlp_best.pth\")['model_state_dict'])\n","# neu_mf_model = NeuMF(gmf_model, mlp_model, n_input, device)\n","# optimizer = optim.Adam(list(neu_mf_model.gmf_model.parameters()) + list(neu_mf_model.mlp_model.parameters()) + list(neu_mf_model.net.parameters()), lr=1e-4, weight_decay=5e-4)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"UCcmnGP33WZJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622482152084,"user_tz":240,"elapsed":8473070,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"055c2a49-9d35-48b1-8488-9399d68b7e27"},"source":["## TODO: train the model using train_data and validate against val_data\n","import math\n","\n","# device = gpu\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","## model params\n","mlp_hidden_size = 150\n","gmf_hidden_size = 5\n","epochs = 80\n","batch_size = 256\n","lr = 1e-3\n","loss_fn = nn.BCEWithLogitsLoss()\n","label_pred = 'pred_y'\n","alpha=0.5\n","k=5\n","\n","# n_users, n_beers, mlp_hidden_size, gmf_hidden_size, device\n","neu_mf = NeuMF(gmf_model=gmf_model, mlp_model=mlp_model, n_input=30, device=device).to(device)\n","\n","optimizer = optim.RMSprop(list(neu_mf.gmf_model.parameters()) + list(neu_mf.mlp_model.parameters()) + list(neu_mf.net.parameters()), lr=lr, weight_decay=5e-4, momentum=0.9)\n","#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [15,60], gamma=0.1)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=1e-5, eps=1e-08, verbose=True)\n","l = []\n","val_p = []\n","train_p = []\n","best_p = 0\n","\n","for i in range(epochs):\n","    print(\"epoch: \",i)\n","    s = 0\n","    train_data_ = train_data.sample(frac=1)\n","    for bid in range(len(train_data_) // batch_size):\n","        if (bid % 1000 == 0):\n","          print(\"iteration: \",i, \", batch: \", bid)\n","        data = train_data_[bid * batch_size : (bid + 1) * batch_size]\n","        optimizer.zero_grad()\n","        \n","        loss = neu_mf.loss(data, loss_fn)\n","        loss.backward()\n","        optimizer.step()\n","        s += loss\n","        \n","    l.append(s.item()/(len(data) // batch_size))\n","    scheduler.step(s.item()/(len(data) // batch_size))\n","    # evaluate precision at 10 of the model\n","  \n","    neu_mf.eval()\n","    with torch.no_grad():\n","      val = val_data.copy()\n","      val[label_pred] = (neu_mf.predict(val).cpu().detach().numpy() > 0.5).astype(int)\n","      val_prec, _ = precision_recall_at_k(val, label_pred=label_pred, threshold=threshold, k=k)\n","      val_prec = sum(prec for prec in val_prec.values()) / len(val_prec)\n","      train = train_data_.copy()\n","      train[label_pred] = (neu_mf.predict(train).cpu().detach().numpy() > 0.5).astype(int)\n","      train_prec, _ = precision_recall_at_k(train, label_pred=label_pred, threshold=threshold, k=k)\n","      train_prec = sum(prec for prec in train_prec.values()) / len(train_prec)\n","\n","      train_p.append(train_prec)\n","      val_p.append(val_prec)\n","    neu_mf.train()\n","    \n","    print(\"Train precision at 5: \", train_prec)\n","    print(\"Validation precision at 5: \", val_prec)\n","    print(\"Current best validation precision at 5: \", best_p)\n","    if val_prec > best_p:\n","      print(\"Validation precision better than best current precision. Saving model to best_state_dict...\")\n","      best_state_dict = {\n","           'model_state_dict': neu_mf.state_dict()\n","           }\n","      best_p = val_prec\n","\n","      \n","    print(\"Average Loss for the current iteration: \", l[i])\n","    print(\"-----------------------------------\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["epoch:  0\n","iteration:  0 , batch:  0\n","iteration:  0 , batch:  1000\n","iteration:  0 , batch:  2000\n","iteration:  0 , batch:  3000\n","Train precision at 5:  0.7150766322328137\n","Validation precision at 5:  0.6894504084801973\n","Current best validation precision at 5:  0\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2270.116455078125\n","-----------------------------------\n","epoch:  1\n","iteration:  1 , batch:  0\n","iteration:  1 , batch:  1000\n","iteration:  1 , batch:  2000\n","iteration:  1 , batch:  3000\n","Train precision at 5:  0.6993439560687026\n","Validation precision at 5:  0.682491616591287\n","Current best validation precision at 5:  0.6894504084801973\n","Average Loss for the current iteration:  2264.05859375\n","-----------------------------------\n","epoch:  2\n","iteration:  2 , batch:  0\n","iteration:  2 , batch:  1000\n","iteration:  2 , batch:  2000\n","iteration:  2 , batch:  3000\n","Train precision at 5:  0.7164044741521091\n","Validation precision at 5:  0.6906780996106648\n","Current best validation precision at 5:  0.6894504084801973\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2262.819091796875\n","-----------------------------------\n","epoch:  3\n","iteration:  3 , batch:  0\n","iteration:  3 , batch:  1000\n","iteration:  3 , batch:  2000\n","iteration:  3 , batch:  3000\n","Train precision at 5:  0.6981601512389531\n","Validation precision at 5:  0.6872032048252567\n","Current best validation precision at 5:  0.6906780996106648\n","Average Loss for the current iteration:  2262.935791015625\n","-----------------------------------\n","epoch:  4\n","iteration:  4 , batch:  0\n","iteration:  4 , batch:  1000\n","iteration:  4 , batch:  2000\n","iteration:  4 , batch:  3000\n","Train precision at 5:  0.7105720973150742\n","Validation precision at 5:  0.6912913825310165\n","Current best validation precision at 5:  0.6906780996106648\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2261.927734375\n","-----------------------------------\n","epoch:  5\n","iteration:  5 , batch:  0\n","iteration:  5 , batch:  1000\n","iteration:  5 , batch:  2000\n","iteration:  5 , batch:  3000\n","Train precision at 5:  0.6903123804379754\n","Validation precision at 5:  0.6807035311592878\n","Current best validation precision at 5:  0.6912913825310165\n","Average Loss for the current iteration:  2264.052490234375\n","-----------------------------------\n","epoch:  6\n","iteration:  6 , batch:  0\n","iteration:  6 , batch:  1000\n","iteration:  6 , batch:  2000\n","iteration:  6 , batch:  3000\n","Train precision at 5:  0.6993664618639437\n","Validation precision at 5:  0.6855906645961476\n","Current best validation precision at 5:  0.6912913825310165\n","Average Loss for the current iteration:  2263.560791015625\n","-----------------------------------\n","epoch:  7\n","iteration:  7 , batch:  0\n","iteration:  7 , batch:  1000\n","iteration:  7 , batch:  2000\n","iteration:  7 , batch:  3000\n","Epoch     8: reducing learning rate of group 0 to 1.0000e-04.\n","Train precision at 5:  0.7085083158913549\n","Validation precision at 5:  0.6905126820156342\n","Current best validation precision at 5:  0.6912913825310165\n","Average Loss for the current iteration:  2263.318359375\n","-----------------------------------\n","epoch:  8\n","iteration:  8 , batch:  0\n","iteration:  8 , batch:  1000\n","iteration:  8 , batch:  2000\n","iteration:  8 , batch:  3000\n","Train precision at 5:  0.7291022438277918\n","Validation precision at 5:  0.7027794657124372\n","Current best validation precision at 5:  0.6912913825310165\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2149.334716796875\n","-----------------------------------\n","epoch:  9\n","iteration:  9 , batch:  0\n","iteration:  9 , batch:  1000\n","iteration:  9 , batch:  2000\n","iteration:  9 , batch:  3000\n","Train precision at 5:  0.7221569554160294\n","Validation precision at 5:  0.7008934800711349\n","Current best validation precision at 5:  0.7027794657124372\n","Average Loss for the current iteration:  2114.66455078125\n","-----------------------------------\n","epoch:  10\n","iteration:  10 , batch:  0\n","iteration:  10 , batch:  1000\n","iteration:  10 , batch:  2000\n","iteration:  10 , batch:  3000\n","Train precision at 5:  0.7243040082821426\n","Validation precision at 5:  0.699594895685655\n","Current best validation precision at 5:  0.7027794657124372\n","Average Loss for the current iteration:  2110.7353515625\n","-----------------------------------\n","epoch:  11\n","iteration:  11 , batch:  0\n","iteration:  11 , batch:  1000\n","iteration:  11 , batch:  2000\n","iteration:  11 , batch:  3000\n","Train precision at 5:  0.7266401098282935\n","Validation precision at 5:  0.7033938739225523\n","Current best validation precision at 5:  0.7027794657124372\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2109.82568359375\n","-----------------------------------\n","epoch:  12\n","iteration:  12 , batch:  0\n","iteration:  12 , batch:  1000\n","iteration:  12 , batch:  2000\n","iteration:  12 , batch:  3000\n","Train precision at 5:  0.7216010622735453\n","Validation precision at 5:  0.7021583057637508\n","Current best validation precision at 5:  0.7033938739225523\n","Average Loss for the current iteration:  2109.171875\n","-----------------------------------\n","epoch:  13\n","iteration:  13 , batch:  0\n","iteration:  13 , batch:  1000\n","iteration:  13 , batch:  2000\n","iteration:  13 , batch:  3000\n","Train precision at 5:  0.7214716539509042\n","Validation precision at 5:  0.7025589089190634\n","Current best validation precision at 5:  0.7033938739225523\n","Average Loss for the current iteration:  2110.858154296875\n","-----------------------------------\n","epoch:  14\n","iteration:  14 , batch:  0\n","iteration:  14 , batch:  1000\n","iteration:  14 , batch:  2000\n","iteration:  14 , batch:  3000\n","Train precision at 5:  0.7233576395922022\n","Validation precision at 5:  0.7038113564242958\n","Current best validation precision at 5:  0.7033938739225523\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2111.113037109375\n","-----------------------------------\n","epoch:  15\n","iteration:  15 , batch:  0\n","iteration:  15 , batch:  1000\n","iteration:  15 , batch:  2000\n","iteration:  15 , batch:  3000\n","Train precision at 5:  0.7252211194382671\n","Validation precision at 5:  0.7037708459928599\n","Current best validation precision at 5:  0.7038113564242958\n","Average Loss for the current iteration:  2110.376708984375\n","-----------------------------------\n","epoch:  16\n","iteration:  16 , batch:  0\n","iteration:  16 , batch:  1000\n","iteration:  16 , batch:  2000\n","iteration:  16 , batch:  3000\n","Epoch    17: reducing learning rate of group 0 to 1.0000e-05.\n","Train precision at 5:  0.7176377917313816\n","Validation precision at 5:  0.7011725519321386\n","Current best validation precision at 5:  0.7038113564242958\n","Average Loss for the current iteration:  2110.337890625\n","-----------------------------------\n","epoch:  17\n","iteration:  17 , batch:  0\n","iteration:  17 , batch:  1000\n","iteration:  17 , batch:  2000\n","iteration:  17 , batch:  3000\n","Train precision at 5:  0.7228962707897384\n","Validation precision at 5:  0.7039486417752737\n","Current best validation precision at 5:  0.7038113564242958\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2075.18212890625\n","-----------------------------------\n","epoch:  18\n","iteration:  18 , batch:  0\n","iteration:  18 , batch:  1000\n","iteration:  18 , batch:  2000\n","iteration:  18 , batch:  3000\n","Train precision at 5:  0.722460783651803\n","Validation precision at 5:  0.7058841401661091\n","Current best validation precision at 5:  0.7039486417752737\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2068.060546875\n","-----------------------------------\n","epoch:  19\n","iteration:  19 , batch:  0\n","iteration:  19 , batch:  1000\n","iteration:  19 , batch:  2000\n","iteration:  19 , batch:  3000\n","Train precision at 5:  0.7241307136587801\n","Validation precision at 5:  0.7050120406004711\n","Current best validation precision at 5:  0.7058841401661091\n","Average Loss for the current iteration:  2062.50146484375\n","-----------------------------------\n","epoch:  20\n","iteration:  20 , batch:  0\n","iteration:  20 , batch:  1000\n","iteration:  20 , batch:  2000\n","iteration:  20 , batch:  3000\n","Train precision at 5:  0.7214705286611383\n","Validation precision at 5:  0.705251727319802\n","Current best validation precision at 5:  0.7058841401661091\n","Average Loss for the current iteration:  2053.79345703125\n","-----------------------------------\n","epoch:  21\n","iteration:  21 , batch:  0\n","iteration:  21 , batch:  1000\n","iteration:  21 , batch:  2000\n","iteration:  21 , batch:  3000\n","Train precision at 5:  0.7283505502667046\n","Validation precision at 5:  0.7078635248576668\n","Current best validation precision at 5:  0.7058841401661091\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2048.316650390625\n","-----------------------------------\n","epoch:  22\n","iteration:  22 , batch:  0\n","iteration:  22 , batch:  1000\n","iteration:  22 , batch:  2000\n","iteration:  22 , batch:  3000\n","Train precision at 5:  0.7282278936826339\n","Validation precision at 5:  0.7084137915513411\n","Current best validation precision at 5:  0.7078635248576668\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2039.4190673828125\n","-----------------------------------\n","epoch:  23\n","iteration:  23 , batch:  0\n","iteration:  23 , batch:  1000\n","iteration:  23 , batch:  2000\n","iteration:  23 , batch:  3000\n","Train precision at 5:  0.738259851911876\n","Validation precision at 5:  0.7116962617874252\n","Current best validation precision at 5:  0.7084137915513411\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2034.710205078125\n","-----------------------------------\n","epoch:  24\n","iteration:  24 , batch:  0\n","iteration:  24 , batch:  1000\n","iteration:  24 , batch:  2000\n","iteration:  24 , batch:  3000\n","Train precision at 5:  0.7388697589629423\n","Validation precision at 5:  0.7126640109828439\n","Current best validation precision at 5:  0.7116962617874252\n","Validation precision better than best current precision. Saving model to best_state_dict...\n","Average Loss for the current iteration:  2029.86669921875\n","-----------------------------------\n","epoch:  25\n","iteration:  25 , batch:  0\n","iteration:  25 , batch:  1000\n","iteration:  25 , batch:  2000\n","iteration:  25 , batch:  3000\n","Train precision at 5:  0.734216685796604\n","Validation precision at 5:  0.7109018072153737\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2023.2872314453125\n","-----------------------------------\n","epoch:  26\n","iteration:  26 , batch:  0\n","iteration:  26 , batch:  1000\n","iteration:  26 , batch:  2000\n","iteration:  26 , batch:  3000\n","Train precision at 5:  0.7372808498188363\n","Validation precision at 5:  0.7112011342920964\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2019.5455322265625\n","-----------------------------------\n","epoch:  27\n","iteration:  27 , batch:  0\n","iteration:  27 , batch:  1000\n","iteration:  27 , batch:  2000\n","iteration:  27 , batch:  3000\n","Train precision at 5:  0.7375441676231734\n","Validation precision at 5:  0.7119820853870036\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2015.0809326171875\n","-----------------------------------\n","epoch:  28\n","iteration:  28 , batch:  0\n","iteration:  28 , batch:  1000\n","iteration:  28 , batch:  2000\n","iteration:  28 , batch:  3000\n","Train precision at 5:  0.7348040870524253\n","Validation precision at 5:  0.7101208561204669\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2012.987060546875\n","-----------------------------------\n","epoch:  29\n","iteration:  29 , batch:  0\n","iteration:  29 , batch:  1000\n","iteration:  29 , batch:  2000\n","iteration:  29 , batch:  3000\n","Train precision at 5:  0.7484358472306721\n","Validation precision at 5:  0.7126460063466488\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2010.8060302734375\n","-----------------------------------\n","epoch:  30\n","iteration:  30 , batch:  0\n","iteration:  30 , batch:  1000\n","iteration:  30 , batch:  2000\n","iteration:  30 , batch:  3000\n","Train precision at 5:  0.7430985978889647\n","Validation precision at 5:  0.7122555307991968\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2011.1700439453125\n","-----------------------------------\n","epoch:  31\n","iteration:  31 , batch:  0\n","iteration:  31 , batch:  1000\n","iteration:  31 , batch:  2000\n","iteration:  31 , batch:  3000\n","Train precision at 5:  0.743722008417174\n","Validation precision at 5:  0.7121587558796546\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2009.068115234375\n","-----------------------------------\n","epoch:  32\n","iteration:  32 , batch:  0\n","iteration:  32 , batch:  1000\n","iteration:  32 , batch:  2000\n","iteration:  32 , batch:  3000\n","Train precision at 5:  0.7419305471158886\n","Validation precision at 5:  0.7108129093241665\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2006.7813720703125\n","-----------------------------------\n","epoch:  33\n","iteration:  33 , batch:  0\n","iteration:  33 , batch:  1000\n","iteration:  33 , batch:  2000\n","iteration:  33 , batch:  3000\n","Train precision at 5:  0.7466646411451046\n","Validation precision at 5:  0.7111685008889936\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2005.264892578125\n","-----------------------------------\n","epoch:  34\n","iteration:  34 , batch:  0\n","iteration:  34 , batch:  1000\n","iteration:  34 , batch:  2000\n","iteration:  34 , batch:  3000\n","Train precision at 5:  0.7442171359125036\n","Validation precision at 5:  0.7114183152161834\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2004.389404296875\n","-----------------------------------\n","epoch:  35\n","iteration:  35 , batch:  0\n","iteration:  35 , batch:  1000\n","iteration:  35 , batch:  2000\n","iteration:  35 , batch:  3000\n","Train precision at 5:  0.7448067877478539\n","Validation precision at 5:  0.7104393131231445\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2002.415771484375\n","-----------------------------------\n","epoch:  36\n","iteration:  36 , batch:  0\n","iteration:  36 , batch:  1000\n","iteration:  36 , batch:  2000\n","iteration:  36 , batch:  3000\n","Train precision at 5:  0.7492021695586704\n","Validation precision at 5:  0.7120777350167828\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2001.1834716796875\n","-----------------------------------\n","epoch:  37\n","iteration:  37 , batch:  0\n","iteration:  37 , batch:  1000\n","iteration:  37 , batch:  2000\n","iteration:  37 , batch:  3000\n","Train precision at 5:  0.7516598023991269\n","Validation precision at 5:  0.710293025454069\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  2002.44189453125\n","-----------------------------------\n","epoch:  38\n","iteration:  38 , batch:  0\n","iteration:  38 , batch:  1000\n","iteration:  38 , batch:  2000\n","iteration:  38 , batch:  3000\n","Train precision at 5:  0.7441124839646298\n","Validation precision at 5:  0.7106261112236553\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1998.9384765625\n","-----------------------------------\n","epoch:  39\n","iteration:  39 , batch:  0\n","iteration:  39 , batch:  1000\n","iteration:  39 , batch:  2000\n","iteration:  39 , batch:  3000\n","Train precision at 5:  0.7497715661782987\n","Validation precision at 5:  0.7119798348074786\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1998.4482421875\n","-----------------------------------\n","epoch:  40\n","iteration:  40 , batch:  0\n","iteration:  40 , batch:  1000\n","iteration:  40 , batch:  2000\n","iteration:  40 , batch:  3000\n","Train precision at 5:  0.7479553485022497\n","Validation precision at 5:  0.7110424684356375\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1997.7760009765625\n","-----------------------------------\n","epoch:  41\n","iteration:  41 , batch:  0\n","iteration:  41 , batch:  1000\n","iteration:  41 , batch:  2000\n","iteration:  41 , batch:  3000\n","Train precision at 5:  0.744259896923466\n","Validation precision at 5:  0.7107633965746339\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1994.6873779296875\n","-----------------------------------\n","epoch:  42\n","iteration:  42 , batch:  0\n","iteration:  42 , batch:  1000\n","iteration:  42 , batch:  2000\n","iteration:  42 , batch:  3000\n","Train precision at 5:  0.7567742443679303\n","Validation precision at 5:  0.7100207053316366\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1994.437744140625\n","-----------------------------------\n","epoch:  43\n","iteration:  43 , batch:  0\n","iteration:  43 , batch:  1000\n","iteration:  43 , batch:  2000\n","iteration:  43 , batch:  3000\n","Train precision at 5:  0.7568755204465253\n","Validation precision at 5:  0.7102828978462099\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1992.786376953125\n","-----------------------------------\n","epoch:  44\n","iteration:  44 , batch:  0\n","iteration:  44 , batch:  1000\n","iteration:  44 , batch:  2000\n","iteration:  44 , batch:  3000\n","Train precision at 5:  0.7514358697364637\n","Validation precision at 5:  0.7097855197713573\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1989.8792724609375\n","-----------------------------------\n","epoch:  45\n","iteration:  45 , batch:  0\n","iteration:  45 , batch:  1000\n","iteration:  45 , batch:  2000\n","iteration:  45 , batch:  3000\n","Train precision at 5:  0.7475097337564509\n","Validation precision at 5:  0.7101264825692781\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1990.0648193359375\n","-----------------------------------\n","epoch:  46\n","iteration:  46 , batch:  0\n","iteration:  46 , batch:  1000\n","iteration:  46 , batch:  2000\n","iteration:  46 , batch:  3000\n","Train precision at 5:  0.7552967389102762\n","Validation precision at 5:  0.7111167375599375\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1987.4515380859375\n","-----------------------------------\n","epoch:  47\n","iteration:  47 , batch:  0\n","iteration:  47 , batch:  1000\n","iteration:  47 , batch:  2000\n","iteration:  47 , batch:  3000\n","Train precision at 5:  0.7540656719105236\n","Validation precision at 5:  0.7086951139918678\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1986.4022216796875\n","-----------------------------------\n","epoch:  48\n","iteration:  48 , batch:  0\n","iteration:  48 , batch:  1000\n","iteration:  48 , batch:  2000\n","iteration:  48 , batch:  3000\n","Train precision at 5:  0.7534490131208879\n","Validation precision at 5:  0.7110739765489769\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1984.437744140625\n","-----------------------------------\n","epoch:  49\n","iteration:  49 , batch:  0\n","iteration:  49 , batch:  1000\n","iteration:  49 , batch:  2000\n","iteration:  49 , batch:  3000\n","Train precision at 5:  0.7590867148290794\n","Validation precision at 5:  0.7094490581324852\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1983.8963623046875\n","-----------------------------------\n","epoch:  50\n","iteration:  50 , batch:  0\n","iteration:  50 , batch:  1000\n","iteration:  50 , batch:  2000\n","iteration:  50 , batch:  3000\n","Train precision at 5:  0.754452771588694\n","Validation precision at 5:  0.7099993248261586\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1983.049560546875\n","-----------------------------------\n","epoch:  51\n","iteration:  51 , batch:  0\n","iteration:  51 , batch:  1000\n","iteration:  51 , batch:  2000\n","iteration:  51 , batch:  3000\n","Train precision at 5:  0.7558121216213255\n","Validation precision at 5:  0.711188756104712\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1983.054443359375\n","-----------------------------------\n","epoch:  52\n","iteration:  52 , batch:  0\n","iteration:  52 , batch:  1000\n","iteration:  52 , batch:  2000\n","iteration:  52 , batch:  3000\n","Train precision at 5:  0.7507246866068125\n","Validation precision at 5:  0.7102581414714451\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1983.1365966796875\n","-----------------------------------\n","epoch:  53\n","iteration:  53 , batch:  0\n","iteration:  53 , batch:  1000\n","iteration:  53 , batch:  2000\n","iteration:  53 , batch:  3000\n","Train precision at 5:  0.7560889429028043\n","Validation precision at 5:  0.7105946031103162\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1980.9593505859375\n","-----------------------------------\n","epoch:  54\n","iteration:  54 , batch:  0\n","iteration:  54 , batch:  1000\n","iteration:  54 , batch:  2000\n","iteration:  54 , batch:  3000\n","Train precision at 5:  0.7565255553305059\n","Validation precision at 5:  0.711103234082792\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1981.9923095703125\n","-----------------------------------\n","epoch:  55\n","iteration:  55 , batch:  0\n","iteration:  55 , batch:  1000\n","iteration:  55 , batch:  2000\n","iteration:  55 , batch:  3000\n","Train precision at 5:  0.7496342808273235\n","Validation precision at 5:  0.7087716336956926\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1981.102294921875\n","-----------------------------------\n","epoch:  56\n","iteration:  56 , batch:  0\n","iteration:  56 , batch:  1000\n","iteration:  56 , batch:  2000\n","iteration:  56 , batch:  3000\n","Train precision at 5:  0.7508180856570643\n","Validation precision at 5:  0.7091283505502832\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1979.5445556640625\n","-----------------------------------\n","epoch:  57\n","iteration:  57 , batch:  0\n","iteration:  57 , batch:  1000\n","iteration:  57 , batch:  2000\n","iteration:  57 , batch:  3000\n","Train precision at 5:  0.7555229221524655\n","Validation precision at 5:  0.7087209956563975\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1980.3128662109375\n","-----------------------------------\n","epoch:  58\n","iteration:  58 , batch:  0\n","iteration:  58 , batch:  1000\n","iteration:  58 , batch:  2000\n","iteration:  58 , batch:  3000\n","Train precision at 5:  0.7506076564715503\n","Validation precision at 5:  0.7084610537213497\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1981.26171875\n","-----------------------------------\n","epoch:  59\n","iteration:  59 , batch:  0\n","iteration:  59 , batch:  1000\n","iteration:  59 , batch:  2000\n","iteration:  59 , batch:  3000\n","Train precision at 5:  0.7572142326649177\n","Validation precision at 5:  0.7125863659892582\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1981.1341552734375\n","-----------------------------------\n","epoch:  60\n","iteration:  60 , batch:  0\n","iteration:  60 , batch:  1000\n","iteration:  60 , batch:  2000\n","iteration:  60 , batch:  3000\n","Train precision at 5:  0.7560203002273169\n","Validation precision at 5:  0.7107667724439206\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1978.7586669921875\n","-----------------------------------\n","epoch:  61\n","iteration:  61 , batch:  0\n","iteration:  61 , batch:  1000\n","iteration:  61 , batch:  2000\n","iteration:  61 , batch:  3000\n","Train precision at 5:  0.7593207750995956\n","Validation precision at 5:  0.7104640694979111\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1979.46142578125\n","-----------------------------------\n","epoch:  62\n","iteration:  62 , batch:  0\n","iteration:  62 , batch:  1000\n","iteration:  62 , batch:  2000\n","iteration:  62 , batch:  3000\n","Train precision at 5:  0.7489726104471973\n","Validation precision at 5:  0.707384151419006\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1978.9801025390625\n","-----------------------------------\n","epoch:  63\n","iteration:  63 , batch:  0\n","iteration:  63 , batch:  1000\n","iteration:  63 , batch:  2000\n","iteration:  63 , batch:  3000\n","Train precision at 5:  0.7544212634753523\n","Validation precision at 5:  0.7092363783674457\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1977.9810791015625\n","-----------------------------------\n","epoch:  64\n","iteration:  64 , batch:  0\n","iteration:  64 , batch:  1000\n","iteration:  64 , batch:  2000\n","iteration:  64 , batch:  3000\n","Train precision at 5:  0.7577149866090607\n","Validation precision at 5:  0.7085972137825655\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1976.1317138671875\n","-----------------------------------\n","epoch:  65\n","iteration:  65 , batch:  0\n","iteration:  65 , batch:  1000\n","iteration:  65 , batch:  2000\n","iteration:  65 , batch:  3000\n","Train precision at 5:  0.7542400918236529\n","Validation precision at 5:  0.7083361465577538\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1976.560546875\n","-----------------------------------\n","epoch:  66\n","iteration:  66 , batch:  0\n","iteration:  66 , batch:  1000\n","iteration:  66 , batch:  2000\n","iteration:  66 , batch:  3000\n","Train precision at 5:  0.7534107532689746\n","Validation precision at 5:  0.7090867148290841\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1975.4521484375\n","-----------------------------------\n","epoch:  67\n","iteration:  67 , batch:  0\n","iteration:  67 , batch:  1000\n","iteration:  67 , batch:  2000\n","iteration:  67 , batch:  3000\n","Train precision at 5:  0.7608275380910637\n","Validation precision at 5:  0.7095728400063176\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1976.5633544921875\n","-----------------------------------\n","epoch:  68\n","iteration:  68 , batch:  0\n","iteration:  68 , batch:  1000\n","iteration:  68 , batch:  2000\n","iteration:  68 , batch:  3000\n","Train precision at 5:  0.7589775617221515\n","Validation precision at 5:  0.7093714131388991\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1974.4530029296875\n","-----------------------------------\n","epoch:  69\n","iteration:  69 , batch:  0\n","iteration:  69 , batch:  1000\n","iteration:  69 , batch:  2000\n","iteration:  69 , batch:  3000\n","Train precision at 5:  0.7577678752278781\n","Validation precision at 5:  0.7098023991177881\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1973.527587890625\n","-----------------------------------\n","epoch:  70\n","iteration:  70 , batch:  0\n","iteration:  70 , batch:  1000\n","iteration:  70 , batch:  2000\n","iteration:  70 , batch:  3000\n","Train precision at 5:  0.7585747079873134\n","Validation precision at 5:  0.712221772106332\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1973.5562744140625\n","-----------------------------------\n","epoch:  71\n","iteration:  71 , batch:  0\n","iteration:  71 , batch:  1000\n","iteration:  71 , batch:  2000\n","iteration:  71 , batch:  3000\n","Train precision at 5:  0.7519017396979847\n","Validation precision at 5:  0.7082900096775071\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1973.7705078125\n","-----------------------------------\n","epoch:  72\n","iteration:  72 , batch:  0\n","iteration:  72 , batch:  1000\n","iteration:  72 , batch:  2000\n","iteration:  72 , batch:  3000\n","Train precision at 5:  0.7520435262080066\n","Validation precision at 5:  0.7091666104021945\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1971.643798828125\n","-----------------------------------\n","epoch:  73\n","iteration:  73 , batch:  0\n","iteration:  73 , batch:  1000\n","iteration:  73 , batch:  2000\n","iteration:  73 , batch:  3000\n","Train precision at 5:  0.7517847095627209\n","Validation precision at 5:  0.709146355186476\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1971.7630615234375\n","-----------------------------------\n","epoch:  74\n","iteration:  74 , batch:  0\n","iteration:  74 , batch:  1000\n","iteration:  74 , batch:  2000\n","iteration:  74 , batch:  3000\n","Train precision at 5:  0.7647142889294052\n","Validation precision at 5:  0.7120068417617682\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1970.728271484375\n","-----------------------------------\n","epoch:  75\n","iteration:  75 , batch:  0\n","iteration:  75 , batch:  1000\n","iteration:  75 , batch:  2000\n","iteration:  75 , batch:  3000\n","Train precision at 5:  0.7620023406027093\n","Validation precision at 5:  0.7113789300745091\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1969.83544921875\n","-----------------------------------\n","epoch:  76\n","iteration:  76 , batch:  0\n","iteration:  76 , batch:  1000\n","iteration:  76 , batch:  2000\n","iteration:  76 , batch:  3000\n","Train precision at 5:  0.7536538158575936\n","Validation precision at 5:  0.709788895640643\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1969.399658203125\n","-----------------------------------\n","epoch:  77\n","iteration:  77 , batch:  0\n","iteration:  77 , batch:  1000\n","iteration:  77 , batch:  2000\n","iteration:  77 , batch:  3000\n","Train precision at 5:  0.7630432336326706\n","Validation precision at 5:  0.7109895798168174\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1969.351318359375\n","-----------------------------------\n","epoch:  78\n","iteration:  78 , batch:  0\n","iteration:  78 , batch:  1000\n","iteration:  78 , batch:  2000\n","iteration:  78 , batch:  3000\n","Train precision at 5:  0.7619550784327052\n","Validation precision at 5:  0.710262642630492\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1969.349365234375\n","-----------------------------------\n","epoch:  79\n","iteration:  79 , batch:  0\n","iteration:  79 , batch:  1000\n","iteration:  79 , batch:  2000\n","iteration:  79 , batch:  3000\n","Train precision at 5:  0.7579096617379072\n","Validation precision at 5:  0.7105878513717431\n","Current best validation precision at 5:  0.7126640109828439\n","Average Loss for the current iteration:  1968.4102783203125\n","-----------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"phSloDMXm6Nv","executionInfo":{"status":"ok","timestamp":1622482153128,"user_tz":240,"elapsed":210,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"d2ec7683-cc66-4f27-c26a-707a5b9c4ab0"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(val_p, color='b', label='Validation')\n","plt.plot(train_p, color='r', label='Training')\n","plt.vlines(np.argmax(val_p),0.67,1.0, linestyles='dashed', label='Best model')\n","plt.ylabel('Precision@5')\n","plt.xlabel('Epochs')\n","plt.legend()\n","plt.show()"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfbA8e8hdEG6ShVUBEEkQMAuKEhTcVFWxFXJuj8R68oqKura0LWhsu7q2sUOiA0VGwiLi+xCqAISQUQJIBCQ3gI5vz/ODZmESTKBTO4A5/M882Tm1jMl99y33PeKquKcc87lVybsAJxzziUmTxDOOeei8gThnHMuKk8QzjnnovIE4ZxzLqqyYQdQUmrXrq2NGzcOOwznnDugzJgxI1NV60Sbd9AkiMaNG5OWlhZ2GM45d0ARkZ8LmudVTM4556LyBOGccy4qTxDOOeeiOmjaIJxzB56srCwyMjLYvn172KEc9CpWrEiDBg0oV65czOt4gnDOhSYjI4OqVavSuHFjRCTscA5aqsratWvJyMigSZMmMa8XtyomEXlFRFaLyLwC5ouIPC0ii0Vkroi0jZjXX0QWBY/+8YrROReu7du3U6tWLU8OcSYi1KpVq9gltXi2QYwAuhcyvwfQNHgMAP4FICI1gXuBk4EOwL0iUiOOcTrnQuTJoXTsy+cctwShqpOBdYUsciHwupr/AtVFpC7QDfhKVdep6m/AVxSeaJxzzsVBmL2Y6gPLIl5nBNMKmr4XERkgImkikrZmzZq4BeqcOzidffbZfPHFF3mmDR8+nGuvvTbq8p06ddpzQW7Pnj1Zv379Xsvcd999DBs2rND9fvjhhyxYsGDP63vuuYfx48cXN/y4O6C7uarqC6qaoqopdepEvVLcJaDx48cn5D+DO/T069ePkSNH5pk2cuRI+vXrV+S648aNo3r16vu03/wJ4oEHHqBLly77tK14CjNBLAcaRrxuEEwraLo7SDz44IM8+OCDYYfhHH369OHTTz9l586dACxdupQVK1bwzjvvkJKSQsuWLbn33nujrtu4cWMyMzMBeOihhzj++OM544wzSE9P37PMiy++SPv27WndujUXX3wxW7du5dtvv2Xs2LEMHjyY5ORkfvzxR1JTUxkzZgwAEyZMoE2bNrRq1YqrrrqKHTt27NnfvffeS9u2bWnVqhULFy6M50cDhNvNdSxwg4iMxBqkN6jqShH5AvhbRMN0V2BIWEE650rHzTfD7Nklu83kZBg+vOD5NWvWpEOHDnz22WdceOGFjBw5kksuuYQ777yTmjVrsnv3bjp37szcuXM56aSTom5jxowZjBw5ktmzZ7Nr1y7atm1Lu3btALjooou4+uqrAbj77rt5+eWXufHGG+nVqxfnn38+ffr0ybOt7du3k5qayoQJEzj++OO58sor+de//sXNN98MQO3atZk5cybPPvssw4YN46WXXiqBT6lg8ezm+g4wFWgmIhki8icRGSgiA4NFxgFLgMXAi8B1AKq6DhgKTA8eDwTTnHOuxEVWM+VUL40ePZq2bdvSpk0b5s+fn6c6KL9vvvmG3r17U7lyZQ4//HB69eq1Z968efM488wzadWqFW+99Rbz588vNJb09HSaNGnC8ccfD0D//v2ZPHnynvkXXXQRAO3atWPp0qX7+pZjFrcShKoWWomnqgpcX8C8V4BX4hGXcy4xFXamH08XXnghgwYNYubMmWzdupWaNWsybNgwpk+fTo0aNUhNTd3nK71TU1P58MMPad26NSNGjGDSpEn7FWuFChUASEpKYteuXfu1rVgc0I3Uzjm3v6pUqcLZZ5/NVVddRb9+/di4cSOHHXYY1apVY9WqVXz22WeFrn/WWWfx4Ycfsm3bNjZt2sTHH3+8Z96mTZuoW7cuWVlZvPXWW3umV61alU2bNu21rWbNmrF06VIWL14MwBtvvEHHjh1L6J0Wnw+14Urd888/H3YIzuXRr18/evfuzciRI2nevDlt2rShefPmNGzYkNNPP73Qddu2bUvfvn1p3bo1RxxxBO3bt98zb+jQoZx88snUqVOHk08+eU9SuPTSS7n66qt5+umn9zROg42X9Oqrr/L73/+eXbt20b59ewYOHLjXPkuLWE3PgS8lJUX9hkHOHVi+//57TjjhhLDDOGRE+7xFZIaqpkRb3quYXKn7+OOP8xTDnXOJyauYXKl74oknALjgggtCjsQ5VxgvQTjnnIvKE4RzzrmoPEE455yLyhOEc865qLyR2pW6N954I+wQnANg7dq1dO7cGYBff/2VpKQkckaGnjZtGuXLly9w3bS0NF5//XWefvrpQvdx2mmn8e2335Zc0KXIE4QrdQ0bNix6IedKQa1atZgdjBB43333UaVKFW699dY983ft2kXZstEPkykpKaSkRL18II8DNTmAVzG5EIwaNYpRo0aFHYZzUaWmpjJw4EBOPvlkbrvtNqZNm8app55KmzZtOO200/YM5z1p0iTOP/98wJLLVVddRadOnTjmmGPylCqqVKmyZ/lOnTrRp08fmjdvzh/+8AdyLlQeN24czZs3p127dtx00017ths2L0G4Uvevf/0LgL59+4YciUsoYYz3XYCMjAy+/fZbkpKS2LhxI9988w1ly5Zl/Pjx3Hnnnbz33nt7rbNw4UImTpzIpk2baNasGddeey3lypXLs8ysWbOYP38+9erV4/TTT2fKlCmkpKRwzTXXMHnyZJo0aRLTzYpKiycI55zL5/e//z1JSUkAbNiwgf79+7No0SJEhKysrKjrnHfeeVSoUIEKFSpwxBFHsGrVKho0aJBnmQ4dOuyZlpyczNKlS6lSpQrHHHMMTZo0AWxcqBdeeCGO7y52niCcc4khrPG+ozjssMP2PP/rX//K2WefzQcffMDSpUvp1KlT1HVyhuKGgofjjmWZROJtEM45V4gNGzZQv359AEaMGFHi22/WrBlLlizZcwOgRGqf8wThnHOFuO222xgyZAht2rSJyxl/pUqVePbZZ+nevTvt2rWjatWqVKtWrcT3sy98uG9X6nJu9F67du2QI3Fh8+G+zebNm6lSpQqqyvXXX0/Tpk0ZNGhQie/Hh/t2Ca927dqeHJyL8OKLL5KcnEzLli3ZsGED11xzTdghAd5I7UKQU4+bmpoaahzOJYpBgwbFpcSwv7wE4UrdiBEj4tLY55wrWXFNECLSXUTSRWSxiNwRZf7RIjJBROaKyCQRaRAxb7eIzA4eY+MZp3POub3FrYpJRJKAZ4BzgQxguoiMVdUFEYsNA15X1ddE5BzgYeCKYN42VU2OV3zOOecKF88SRAdgsaouUdWdwEjgwnzLtAC+Dp5PjDLfOedcSOKZIOoDyyJeZwTTIs0BLgqe9waqikit4HVFEUkTkf+KyO+i7UBEBgTLpK1Zs6YkY3fOHSKSkpJITk6mdevWtG3bdp9HXx0+fDhbt24t4ej2lpqaypgxY/Z7mViE3Uh9K9BRRGYBHYHlwO5g3tFB39zLgOEicmz+lVX1BVVNUdWUnDHcXeIbN24c48aNCzsM5wC7UG327NnMmTOHhx9+mCFDhuzTdkorQZSmeCaI5UDkwP8Ngml7qOoKVb1IVdsAdwXT1gd/lwd/lwCTgDZxjNWVosqVK1O5cuWww3BuLxs3bqRGjRp7Xj/++OO0b9+ek046iXvvvReALVu2cN5559G6dWtOPPFERo0axdNPP82KFSs4++yzOfvss/fabuPGjRkyZAjJycmkpKQwc+ZMunXrxrHHHstzzz0HgKoyePBgTjzxRFq1arVnyA1V5YYbbqBZs2Z06dKF1atX79nujBkz6NixI+3ataNbt26sXLmyRD+PeF4HMR1oKiJNsMRwKVYa2ENEagPrVDUbGAK8EkyvAWxV1R3BMqcDj8UxVleKnn32WQCuu+66kCNxiSbaQHiXXHIJ1113HVu3bqVnz557zU9NTSU1NZXMzEz69OmTZ96kSZOK3Oe2bdtITk5m+/btrFy5kq+/tmbRL7/8kkWLFjFt2jRUlV69ejF58mTWrFlDvXr1+PTTTwEbq6latWo8+eSTTJw4scCLQBs1asTs2bMZNGgQqampTJkyhe3bt3PiiScycOBA3n///T0lmczMTNq3b89ZZ53F1KlTSU9PZ8GCBaxatYoWLVpw1VVXkZWVxY033shHH31EnTp1GDVqFHfddRevvPJKke85VnFLEKq6S0RuAL4AkoBXVHW+iDwApKnqWKAT8LCIKDAZuD5Y/QTgeRHJxko5j+Tr/eQOYKNHjwY8QbjEkFPFBDB16lSuvPJK5s2bx5dffsmXX35JmzZWebF582YWLVrEmWeeyS233MLtt9/O+eefz5lnnhnTfnr16gVAq1at2Lx5M1WrVqVq1apUqFCB9evX85///Id+/fqRlJTEkUceSceOHZk+fTqTJ0/eM71evXqcc845AKSnpzNv3jzOPfdcAHbv3k3dunVL9LOJ65XUqjoOGJdv2j0Rz8cAe7WkqOq3QKt4xuacSzyFnfFXrly50Pm1a9eOqcRQmFNPPZXMzEzWrFmDqjJkyJCow17MnDmTcePGcffdd9O5c2fuueeeKFvLK2eo7zJlyuQZ9rtMmTL7NAigqtKyZUumTp1a7HVjFXYjtXPOJYyFCxeye/duatWqRbdu3XjllVfYvHkzAMuXL2f16tWsWLGCypUrc/nllzN48GBmzpwJQNWqVdm0adM+7/vMM89k1KhR7N69mzVr1jB58mQ6dOjAWWedtWf6ypUrmThxImDDhK9Zs2ZPgsjKymL+/Pn7+Qnk5WMxOecOaTltEGBn5a+99hpJSUl07dqV77//nlNPPRWwe0u/+eabLF68mMGDB1OmTBnKlSu35xa6AwYMoHv37tSrV2/PQbw4evfuzdSpU2ndujUiwmOPPcZRRx1F7969+frrr2nRogWNGjXaE0/58uUZM2YMN910Exs2bGDXrl3cfPPNtGzZsoQ+GR/u24UgpyFyf6sD3IHPh/suXcUd7ttLEK7UeWJw7sDgbRDOOeei8gThSt2wYcMYNmxY2GG4BHGwVHMnun35nD1BuFL3ySef8Mknn4QdhksAFStWZO3atZ4k4kxVWbt2LRUrVizWet4G4ZwLTYMGDcjIyMAH24y/ihUr0qBBg6IXjOAJwjkXmnLlytGkSZOww3AF8Com55xzUXkJwpW6SpUqhR2Ccy4GniBcqfvss8/CDsE5FwOvYnLOOReVJwhX6oYOHcrQoUPDDsM5VwRPEK7UTZgwgQkTJoQdhnOuCJ4gnHPOReUJwjnnXFSeIJxzzkXl3VxdqatVq1bYITjnYuAJwpW69957L+wQnHMx8Com55xzUcU1QYhIdxFJF5HFInJHlPlHi8gEEZkrIpNEpEHEvP4isih49I9nnK50DRkyhCFDhoQdhnOuCHGrYhKRJOAZ4FwgA5guImNVdUHEYsOA11X1NRE5B3gYuEJEagL3AimAAjOCdX+LV7yu9EydOjXsEJxzMYhnCaIDsFhVl6jqTmAkcGG+ZVoAXwfPJ0bM7wZ8parrgqTwFdA9jrE655zLJ54Joj6wLOJ1RjAt0hzgouB5b6CqiNSKcV1EZICIpIlImt9wxDnnSlbYjdS3Ah1FZBbQEVgO7I51ZVV9QVVTVDWlTp068YrROecOSfHs5rocaBjxukEwbQ9VXUFQghCRKsDFqrpeRJYDnfKtOymOsbpSVNzbHjrnwhHPBDEdaCoiTbDEcClwWeQCIlIbWKeq2cAQ4JVg1hfA30SkRvC6azDfHQTefPPNsENwzsUgblVMqroLuAE72H8PjFbV+SLygIj0ChbrBKSLyA/AkcBDwbrrgKFYkpkOPBBMc845V0pEVcOOoUSkpKRoWlpa2GG4GNx8880ADB8+PORInHMiMkNVU6LN86E2XKmbPXt22CE452IQdi8m55xzCcoThHPOuag8QTjnnIvK2yBcqTv++OPDDsE5F4NiJ4jggrbjgSWqur7kQ3IHuxdeeCHsEJxzMSiyiklEno14fgawAHgC+E5EesYxNueccyGKpQRxSsTzocDvVHWmiBwDjAbGxSUyd9AaMGAA4CUJ5xJdcauYDlfVmQCqukREvJHbFdsPP/wQdgjOuRjEkiCai8hcQIDGIlJDVX8LkkP5+IbnnHMuLLEkiBPyvd4S/K0J3FOy4TjnnEsURSYIVf25gOmZwPslHpFzzrmEEFMbhIiUA/4M9MiZBHwDDA1GbXUuZsnJyWGH4JyLQZEJQkQqAp8AbwFdVXV3MP1K4B4ReRdYpKrb4xqpO2j4KK7OHRhiKUHcBoxS1VdF5KWgeytYKQJgGnAxcF8c4nPOOReSWLqpnge8GjxfB/wD6An8HUsOnwPnxyU6d1C6/PLLufzyy8MOwzlXhFgSRKWIdoZzVPWDoDrpI6BzMC8pbhG6g05GRgYZGRlhh+GcK0IsCWKBiHQInn8kIu+KyABgFPCxiDQDlsYrQOecc+GIpQ3iEeAfItJVVYeKyElAc+BBYBHWgD0kjjE655wLQZElCFWdDTwOTBaRPwG7gblAW2AS8LyqTotnkM4550pfTNdBqOpYEfk3cAlwbTB5HtBDVdfFKzh3cDr11FPDDsE5FwNR1fhtXKQ71tspCXhJVR/JN78R8BpQPVjmDlUdJyKNge+B9GDR/6rqwML2lZKSomlpaSX7Bpxz7iAnIjNUNSXavJhHcxWR07FrHY6OXE9Vjylg+STgGeBcIAOYLiJjVXVBxGJ3A6NV9V8i0gIbOrxxMO9HVfVLbp1zLiTFGe77ZWAQMANrhyhKB2Cxqi4BEJGRwIXYDYdyKHB48LwasKIY8bgD1MUXXwzAe++9F3IkzrnCFCdBbFDVz4qxfH1gWcTrDODkfMvcB3wpIjcChwFdIuY1EZFZwEbgblX9Jv8Ogu62AwAaNWpUjNBcmNauXRt2CM65GBTnhj8TReRxETlVRNrmPPZz//2AEaraALs6+43gPhMrgUaq2gb4C/C2iByef2VVfUFVU1Q1pU6dOvsZinPOuUjFKUHknP1HNmYocE4Byy8HGka8bhBMi/QnoDuAqk4NBgasraqrgR3B9Bki8iNwPOCt0M45V0piThCqenYxtz0daCoiTbDEcClwWb5lfgE6AyNE5ASgIrBGROoA61R1dzA4YFNgSTH375xzbj8UpxdTNeBe4Kxg0r+BB1R1Q7TlVXWXiNwAfIF1YX1FVeeLyANAmqqOBW4BXhSRQVhpJFVVVUTOAh4QkSwgGxjo11scPDp37hx2CM65GMR8HYSIvIddHPdaMOkKoLWqXhSn2IrFr4NwzrniK5HrIIBjVfXiiNf3i8js/QvNOedcoipOL6ZtInJGzovgwrltJR+SO9j16NGDHj16FL2gcy5UxSlBXAu8FrRFCHbzoNR4BOUObtu2+XmFcweC4vRimg20zrkeQVU3xi0q55xzoSsyQYjI5ar6poj8Jd90AFT1yTjF5pxzLkSxlCAOC/5WjWcgzjnnEkuRCUJVnw/+3h//cNyh4Pzzzw87BOdcDIpzodxj2G1GtwGfAycBg1T1zTjF5g5St956a9ghOOdiUJxurl2DhunzgaXAccDgeATlnHMufMVJEDmljfOAdwsaYsO5onTq1IlOnTqFHYZzrgjFuQ7iExFZiFUxXRsMqLc9PmE555wLW8wlCFW9AzgNSFHVLGALdoc455xzB6FYroM4R1W/FpGLIqZFLvJ+PAJzzjkXrliqmDoCXwMXRJmneIJwzrmDUizXQdwb/P1j/MNxh4JLLrkk7BCcczEoznUQfwMeU9X1wesawC2qene8gnMHp+uuuy7sEJxzMShON9ceOckBQFV/A3qWfEjuYLd161a2bt0adhjOuSIUp5trkohUUNUdACJSCagQn7DcwaxnTzuvmDRpUriBOOcKVZwE8RYwQUReDV7/kdzbjzrnnDvIFOd+EI+KyBygSzBpqKp+EZ+wnHPOha04JQiA74FdqjpeRCqLSFVV3RSPwJxzzoUr5kZqEbkaGAM8H0yqD3xYxDrdRSRdRBaLyB1R5jcSkYkiMktE5opIz4h5Q4L10kWkW6xxOuecKxnFKUFcD3QA/gegqotE5IiCFhaRJOAZ4FwgA5guImNVdUHEYncDo1X1XyLSAhgHNA6eXwq0BOoB40XkeFXdXYx4XYJKTU0NOwTnXAyKkyB2qOrOnGE2RKQsdiV1QToAi1V1SbD8SGzspsgEocDhwfNqwIrg+YXAyKDH1E8isjjY3tRixOsSlCcI5w4MxbkO4t8icidQSUTOBd4FPi5k+frAsojXGcG0SPcBl4tIBlZ6uLEY6yIiA0QkTUTS1qxZU4y34sKUmZlJZmZm2GE454pQnARxO7AG+A64Bjug7+9V1P2AEaraALvo7g0RKc4Isy+oaoqqptSpU2c/Q3GlpU+fPvTp0yfsMJxzRYipiiloT5ivqs2BF2Pc9nKgYcTrBsG0SH8CugOo6lQRqQjUjnFd55xzcRTT2XrQOJwuIo2Kse3pQFMRaSIi5bFG57H5lvkF6AwgIicAFbFSyljgUhGpICJNgKbAtGLs2znn3H4qTiN1DWC+iEzDbhYEgKr2irawqu4SkRuAL4Ak4BVVnS8iDwBpqjoWuAV4UUQGYQ3WqaqqwX5GYw3au4DrvQeTc86VruIkiL8Wd+OqOg5rq4icdk/E8wXA6QWs+xDwUHH36ZxzrmTEcke5isBA4DisgfplVd0V78Dcwevaa68NOwTnXAxiKUG8BmQB3wA9gBbAn+MZlDu49e3bN+wQnHMxiCVBtFDVVgAi8jLeWOz207JldolLw4YNi1jSORemWBJEVs6ToOE5juG4Q8EVV1wB+P0gnEt0sSSI1iKyMXgu2JXUG4PnqqqHF7yqc865A1WRCUJVk0ojEOecc4mlOENtOOecO4R4gnDOORdVce8o59x+u+WWW8IOwTkXA08QrtRdcMEFYYfgnIuBVzG5Upeenk56enrYYTjniuAlCFfqrrnmGsCvg3Au0XkJwjnnXFSeIJxzzkXlCcI550rb88/De+/BrsQeGNvbIJxzrjRNmQIDB9rzBg3gmmvg6qvhyCOjL//TT/Dii3DccdCyJbRoAVWrlkqoYjdwO/ClpKRoWlpa2GG4GIwfPx6ALl26hByJcxGWLoXDDoM6deK7n65dYc4ceOYZeOEF+OorKF8e3n4bLr4477K7dsFpp8H06Xmnt20LTz8Np0e931qxiMgMVU2JNs+rmFyp69KliyeHMKlCRkbYURRfdjZMngzbtpX8tqdOhRNOsLP4k0+G+++HadMgK6vodYvj228tIQweDH36wJdfwvffQ3IypKbCwoV5l3/4YUsOb78NP/wAH3wAQ4fCmjVwxhlWElm/vmRjjKSqB8WjXbt26g4Ms2bN0lmzZoUdxqFp507Vq65SBdU//lF106awI8orO1s1K2vvaR98oNqqlcWdmhrbtn7+WXXSJNXduwtfbvFi1dq1VY87TvW++1RPOUVVxPZVvrxq27b2mT36qOqtt6r26aOakqLaoYPqwoXFe39du6rWqaO6eXPe6cuWWQwtWuR+J2lpqmXLqvbrt/d2Nm1SHTRItUwZ1aOOUh01yj6nfQCkaQHH1dAP7CX18ARx4OjYsaN27Ngx7DASy5Ilqh072gGiWTPVY4+1A9DcuSW3jw0b7AAFqj162EGwaVM7EOXIzlb95RfVFSsK3s6//606bJjqXXepXnutav/+qv/7X+xx5E8AOdassYNx+fKqycmWCB55RLVdO4v5+ONVf/c7ez5pUsHbz8hQve461XLlbNl27VQnTIi+bGamfQa1aqn+8EPeWN55R3XwYNVzz7WDN6hWqGDfT7dudqCvX9++u1h8+61t47HHos//6iv7Ti67THXbNvst1K2runZtwdtMS7PP7Iwzik6EBfAE4RKKJ4goLr1UtXJlOzvt21f1D3+wg0OVKqqfflr4ulu32oGzZUvVyy9XfeIJ1a+/Vv3pJzvQqKouX67aurVqUpLqSy/ZtEmTVBs0sAPptdeq9uplZ6M5B8KXX867n+xsO8O2Sio7e61VS/Xww1WrVVMtqlS4dq3qRRfZsmPG7D0vOVm1YkXVm26yA3BOLI0bq776qiWWLVtUmzRRbd5cdfv2vNtYv97OqitUsDPvAQNUn39etVEj2063bqpffKG6dKlta9s2O7BWqKD6n/8UHnt2tupvv+U9CM+Zo1qzpsX3yy+Fr69q+69de+/SQ6ShQ3OTGqh+9lnR283KUl21qujlChBaggC6A+nAYuCOKPOfAmYHjx+A9RHzdkfMG1vUvjxBHDg8QeQzd679K955Z97pGRmqbdrYgfjvf49ehZCdbWecoNqli2q9erkH8JxH9ep2EK9SRfXzz/Ouv3at6sUX25lr8+aqV16p+o9/2LZAdeBA1R077MB8ySU27corbb2cg+XPP6s2bGhn1Onp0d/jN9/YMuXKqZ5wgm1n0CCr8lq/3qpsypffO77MzL1LHOPG2fpDh+ZOW7LEzrjLlLGSx48/5s7bts1KPDVq5H4mZcvmlgpGjSr4uynK9On22TZtWnipa+pU29ejjxa+vd27VXv2tGWvuWbf4yqGUBIEkAT8CBwDlAfmYPe3Lmj5G4FXIl5vLs7+PEEcOA6KBLFwoertt6teffXeZ7LF1bu3nVWvW7f3vM2bc6tVBgxQ3bgx7/y//c3mPfhg7rRVq+xM+aWXbPr119tBvbAz/J07877OyrL3B1Ynn5JiSeSxx6InqvR0SxANG+Y9m/71V9UHHrAD97HH2gF1xw7VG2+0bZ92mm2/XDnVjz8u+rPK8fvf25n/Dz+oTpli+65eveCqJFWrYvvqK9UXXrBkfNllqq+/Hvs+CzJliuphh9mjZk1LxBUqWGmoZk0rpdWoYQkpljafdevshKCwkkYJCitBnAp8EfF6CDCkkOW/Bc6NeO0J4iB1wCaI7dut2uX00+1fJynJ/vbvv/dBMztb9cUX7Sx3+vSC64fT0mwb999f8H5377aDtYiVEEaPtu1/9JFNu/TSfW6gLNK779qBr0oV219hZs60s+ljjrG2jiOPzD1jv+wyO0BHGjnStpuUpPr++znnhqQAABsCSURBVMWLa/ly21eLFnYwPu644jcYl6SpU620df31VjK67TZr0L7+eusMcMklqu+9F158hQgrQfQBXop4fQXwzwKWPRpYCSRFTNsFpAH/BX5XwHoDgmXSGjVqFK/Pz5WwKVOm6JQpU0pmY3PmqJ53nv2Np1mzcnvRNGtmZ9IrV9qBPf8Z/M6ddlCIrOapU8faB/LXdffoYWeZ+Q+e0UydavX0oNq5sx1cU1KsDSKeli619oxYfPONJYg2bayq56mn7Ay7oAS2ZInqtGn7Ftc//mGfxVlnWVWU2ycHQoK4HfhHvmn1g7/HAEuBYwvbn5cgDlHnnWc/41jOcPNbvdrO+m65RXXevOjLZGXZwb9cOWs0/eijvAe77Gw78IOdEW/caI2RoHrvvVbd8+abtkytWja9b1+rt58yxV4/8kjsMWdlWfVD1arWiJ2RUbz3fDDZvdsa2nfsCDuSA1rCVzEBs4DTCtnWCKBPYfvzBHHgKLESxJw59hO+4YbcOvKHH46tuuXrr+0AW768NViCavv2dlb66qt20B40yLoQ5hzUCzpL3b49tzdMy5Z5ewpF2rxZ9Z57rG66YkXrjXPEEftW15yZaQnOuf0UVoIoCywBmkQ0UreMslzzoIQgEdNqABWC57WBRYU1cKsniANKibVB9OtnJYd166yXTd++9pM+4wy7pqBZM6unPuooO4N/4w2ru/7rXy2ZNGumOnu2neU/+WRuFVLOo0oV63HzzjtFx7JmjTXCHnZY0V0Tf/7Z2g3AEpJzISosQcR1LCYR6QkMx3o0vaKqD4nIA0FAY4Nl7gMqquodEeudBjwPZGPDgQxX1ZcL25ePxXTg6NSpE7CfNwz68Uc4/nj4y1/g8cdtmio88gi8/jrUrg1169ojM9OGNMjMzF0/NRX++U8beyeHKixaBGXL2pALkfNisW4dbN1qA7DFYtUqOOIIECnefpwrQYWNxeSD9blS1/XMM9ktwoTJk/d9IwMHwquv2kiX9eoVvXx2NsycCePHQ7Nm0Lv3vu/buYNIYQnCh/t28fff/8Jdd8Hy5fDrr3y5YQPrypWDzz6DHj32Xn79elt261bYsgV27LAB1KpXt/krV1pySE2NLTkAlCkDKSn2cM7FxEdzdfG1eTNceiksWACtWsGVV/JS48asL18eeva0KqIdO2zZpUvh+uvhqKPgxBOhQwc4+2zo3h3q14drr7XtDB9uwyAPHhzqW3PuYOclCBdf99wDP/8M33xjwxMDKVddxabt2+HNN+Gpp2DSJEsIb79tZ/r9+0OXLtYGULmyVQ+9/baVGp57DpKS4Pe/txuoOOfixtsgXPxMmwannmp3zHr22ejLjB0LV11l1UnXXAO33FJwI29mJrz0Enzyid1h64QT4he7c4cIb6R2pS8rC9q1s549CxbA4YfvmbXXHeW2brUqo4hlnHOlwxupE9ny5dbQerB1dXz8cfjuO/joo70O/A8++CAQkSAqVy7t6JxzMfAEEabx4+Hcc+Hee+G++8KOZt/99JM1Lm/YYNcQlCsH//kPXHIJ9OoVdnTOuX3kvZjCNGqU/b3/frsBeUGys60hd8AAuOMO2L69VMKLycqVluSmTrWSgIjdM7hr18Lfk3Mu4XkJIiy7d1sDbZ8+Vv/+5z9DzZpw+eU2XxXmz7feO2+9Bb/8Yr16tmyxkseYMdC4cdH7+fVXu95g3DiYN896DrVrV7xYs7OtHeGII+yRY906SwS//goTJti1Cs65g4YniLD897+werUliAsvhPPOswu/tm6FjAx4911YuNC6dHbtakNIXHihJYcrrrCD/Dvv2LxI2dkwfTp8+qklhRkzbHq9epaULrjAehfl7yk0Y4atU60a1Khhj2XL4OuvYeJESwblylm8110Hycl2HcMPP9h+PDk4d9DxXkxhGTwY/v5367p5+OGwaROccw6kpdm1AGedZXX4F11k4wJFWrTIps+fDx07QsWKdvAWsaqeNWtsG6ecYomnZ09o3dpKEKefDscea9clVKli23vlFbsIbefOveNs2BA6d7b9zJ4NI0ZYW0O1anYR3Jgx8LvfFeutp6enA9CsWbN9+OCccyXJu7kmGlVo2tQen32WO/233+xsvEuXvZNCflu2wO23w6xZ1qU053HSSZYUuneHWrX2Xu+zz+D88+3x7rtw222WqLp0yb1Q7bff7FGjhiWTyB5WW7ZYyeXNN+26hX79SuYzcc6FwhNEopk/364cfu45O8iWtn/8A266yaqZMjLg5putW2rZ0qlx/PjjjwG44IILSmV/zrmCFZYgvBcT2Bl9dnb0eRkZ8Ne/2tl5SfnwQ/sbVhfQG2+0BLF6tVUvPfVUqSUHgCeeeIInnnii1PbnnNs3niCWLrVB5IKz2r08+ig8+CB8/nnJ7fPDD619oG7dkttmcQ0fbg3Pf/xjeDE45xKa92Jq0MCGl37uOeslFGnHDquXB+tqWhJVIhkZ1hD9yCP7v639IVL8G+Lsp507YckSyMw8HbCPPWcE7321YoV1sqpc2bZVrRpUrWoFopxH9epQqVIJvAGsoFnGT6vi5ocfYO1aaN++VAu1xaZql/sc7IMAJPBXUErKloWrr7aL1ZYsgWOOyZ338cd2lt2ypV2zsGmTHX0i7dxpR42KFWPb39ix9reYPX8OVOvXWw3WO+/Yx7t7N8BDgN307eSTrafuMcdYp6gtW+xjXrnSCnc//2xJoGVL64x13nnWDv/55zZe36efFlw7mCMpyZp82re3EcTr1LGOWBs32mPLFutdvHWrXYNYv77dU6hZMyvkTZtmSWjiRLuR3amnQrdu9mjTxtbZtMkeS5bY8tOmWW9jyO2P0LSpvecqVSw3V6li22/YMLdDWXGo2mezcKF1bDv6aLtmMf+BdeVK+OILG/y2QwcoXz769rZssY5uc+bY93bssRbzccdZgt2xw97jxo32b5GZaY/ffrPP9pRT9n3EmK1bbTCBJ5+030j16vZeevSw76FKFfvXO/xwu1worJFpfvkF3njDOvMtWWI1tfffH/swYqtW2biVzz9vn2fOTQ/r17fPsGNH+63mnIT89pt9HxkZub+VBg0sMe3cmfu7g9guiyoub6QGGw/p6KNtJNFHH82dft559u28/bZ9c2+8kXshW47evW39adNi21fXrnZ9wfff71usB4iNG61z1BNP2MG4e3e7dKNZM3jqqYFkZ5fnggue5ssv7UCa/2d4xBH2lRx9tN0eYvp0+4hVrUdvVpZ19PrjH6FvX1tn/Xrb16ZNdpDZtcseGRm56//2296xlitn/3CVKtnBc8UKWy9S1apw5pl2wJw82TqPFUQEmje3f/hy5eyseNEiu56wIDVr2vvMyspNVrt2WUey2rXtUalS7gF60yb72W3enHc7devaZTKXXWY/sddes7ut5iTRSpWsp3OHDraP1avt8csvFmNBh4OyZff+TPI77jj79+jb1xJgTgLets0+v+rVc0t5kUlqwgQbJGDJEjtX69zZEtrnn1tyy+/00+HOOy15RCaKX36x76VWLbvsp27d6CVHVesJ/ssvdsBeu9YS3bp1Nq9iRXuUL2+fb06nvh9/tO9eFTp1soP1m2/afp56ykagB9vmjz/a7xEsxuxs+OADWz4ryw4tRx9t72/lSotl+fLc30Lr1vZ5/Pxz9M+6fPm8vdJPOcV6uO8L78UUi4susmsDMjKgQgX71ho0sK6kDz5op7gnnJC3W+q339qvFSA93e6RXJiMDGjSBG69FR5+eN9jTTA//GCjhmRk5B5wFiywf5ALL7Qzw+Tk3OWXLVsGQMOGDQH7x1y71g4iVarYwTpaNc6aNXbQmDbNDiLnnWcH4Fip2j/u5s12xpfzyH9GvWuXDS+Vnm7/tMnJltwiz8xXrYKvvrJlqlSx7VStagemdu3sIJjfpk32mWzZkltSWrHCDg7LllkCKV8+9zYYSUm5Z+pr1+YeaHP2ddRRlnCbN7eD84wZdsuMceNySmp2ELvySrj4YjvYTJxoo7bMnWtx51wcX6+eNcUlJ9vBqWZN+6wWLbLHli25+61a1ebXqWOJ67DD7ID+xhu2/VgOKWXL5iblVass8b7wgh14I7+v+fNzE+Hmzfb8uefs82rTxq7ZzLlWc/78vfeTU1qrXNkeO3bY7zTaaDVJSXYwz58IK1WyRH3EEfZ7vvLK3IqG//3PLiGaNcvO4Nessc8qmsqV7VrYP/85+qHi55/h3/+27+e77+w7zfk+Gje238eyZfbYuDH3/6VqVTtU5b9mNlaeIGLx1Vf2Cb/1lp1+PfaYJYecA/+QIdYVdMUK+6Wo2oVtc+faf/Ejj9jyBcnOtu3/979WKjn22H2PNc5ybt/822/2j9Co0d4H4qwsG6j1uefsDFAk92BzxBF2YLr+er/DZxhWrbLv5rjj7IAbLdnu2hWfOv5ly+xgnZSUm4BzSj7r19tvasOG3FLSli12zjRoUOztRDt32r/pww9b8ipXzq4r7dEDTjvNDp4rV9q/amZmbhXili22bKNGuY+jjsotoR1+eG6C2LHDkkiVKna+WJhdu3L/D44+2v61jz3Wtgl2qFC1ZF6jxv59vvHgCSIW2dn2DR51lJUjW7a0b3PKFJs/b56dYv3zn3bkmzDBLi57+mkrx5ctawf/gjz1lN1e88UX4f/+b9/jLIb16+1ALmIHiQoVCm6X3rbNcuTYsVavH1kdkpRk/0yVKuX+Y2/aZOs0amTVA1ddFXunrFHBIIV9c+qGnNsHu3fbmXuzZns3DbrYhZYgRKQ78HcgCXhJVR/JN/8p4OzgZWXgCFWtHszrD9wdzHtQVV8rbF8lcqHcsGE2BMbLL8Of/mRl3quvzp1/0kn2S/zPf6ylcsUKO4V54gm46y4ru9avv/d2v/vOTqV79LCKyKDidMcOyy1t2li1RGG9Y1auhLvvtuJ0crKt06aNnfWsX2+PdeusmD1zpj1y6jRzlCljIVxzjTX4JiVZyM8+a2917VrbXvfu1mGrYUOrB/3xR/ublWVJolIlSzTnnGPbS0oq3sfcKahHmDRpUvFWdM6VuMISBKoalweWFH4EjgHKA3OAFoUsfyPwSvC8JrAk+FsjeF6jsP21a9dO91tmpmqFCvaoVEl1w4a88x9+2EqLf/+7/X3xRZu+YIG9fuaZvbe5bZtqq1aqRx6punp1nlkPPZRT+LTZqamqo0apLl2qmp1ty+zYofrYY6pVqqiWL696yin2PLfgmvchonrCCap/+IPqo4+q/vOfqk8/bSHffrtq3bq2XIMGquefr5qUpFqmjOrvfqf6xRe2v3jr2LGjduzYMf47cs4VCUjTgo7LBc3Y3wdwKvBFxOshwJBClv8WODd43g94PmLe80C/wvZXIglCVfWKK+xjufzyvef99FPuUfi441R37syd16yZaufOe68zaJCtM25cnsnr1qlWq6bavbvqG2+oXnqpavXquQf62rVVu3WzzYLqBReoLl5s6+7erZqerjpypOprr6l+9JHq5Mmqc+aobt5c+NvbuVP1vfdUu3ZVPeoo1VtuUV2ypHgf0f7yBOFc4igsQcTzOoj6wLKI1xlA1DGhReRooAnwdSHr7lV3IyIDgAEAjRo12v+IwTo2jx4NAwfuPa9xY+u1NGWKdX6ObLm96CJr2F63zrp4gFXqP/WUdbXo0SPPph5/3BrrHn3Uaq4uv9wau2bNsuvo0tKsV0rFitYm0LNn7rplyli7eVGdpqIpV85Cveii4q/rnDu0JMqFcpcCY1R1d3FWUtUXgBfA2iBKJJKUFGuBLaj/5F13WZ/O/A2svXtbt4pPPrF+cJmZ0L+/dY0dNizPor/+atcI9OtnySFH2bLWd759+xJ5J845t1/imSCWAw0jXjcIpkVzKXB9vnU75Vt3UgnGVrjCOtf36LFXaQCwxNKgAbz/vl2pNGCAJYlx4/bqv/e3v1kD9QMPlHDcB4gxY8aEHYJzLgbxHFVmOtBURJqISHksCYzNv5CINMcaoiOvA/wC6CoiNUSkBtA1mJa4RKwU8cUX1vX1gw+sRBF5hRh2Mcxzz1knqeOOCynWkNWuXZvaOZ3EnXMJK24JQlV3ATdgB/bvgdGqOl9EHhCRyHGuLwVGBo0lOeuuA4ZiSWY68EAwrdSpWhfPguaNHm01ToAliO3b7f4KnTvb1T/53HeftSH89a9xCznhjRgxghEjRoQdhnOuKAW1Xh9oj33txbRrl+qdd6ouWxZ9/s03Wy+i7t1VZ8/Onf7jj9YTCKyr6Ny5qpqVpVqrlmrNmqoZGXm2k52tOmyYdSn9y1/2KdSDhvdici5xUEgvpkN+4OIlS+wGa9262YVikd55x26b0KWLjbnSpo01L/ztbzbi4tSp1hupRg0bjyW7TFkrUnz+eZ4L5tavt15Dt95qY7ncf38pv0nnnNsHidKLKTRNm9qo3t26WVfSCRNs/JXvvrMRMc4809qZt2yxLqnDh1stUq9e8Mwz1i5ds6a1Kbz+OqSmnpNn+7NnQ58+1vbw5JNW+xTWUMXOOVcch3wJAmwk79Gj7bqD3r1tsLPevW1EztGjrVNT9erW5rx4sY0m+uGHlhzARmg87TQbpWNd0FKiasMunXKKJZR//9uaJDw5OOcOFJ4gAr162RBM48fb4F8//wxjxtjYfZFybuwReaAvU8bGM1q3zi6T2LLFLoEYMMBGmZw1yxKIc84dSA75KqZI/fvbQf4vf7FBW4tzUG/dGm680Xq4jh9vA9zdf78ljOIOZnewGzduXNghOOdi4MN9R7F2rd2Vqrg2brQLp7Oy7CZ0XbqUSDjOORc3hY3m6iWIKPYlOYANlT1zpt0VLBFvDJIonn32WQCuu+66kCNxzhXG2yBK2JFHenIoyujRoxk9enTYYTjniuAJwjnnXFSeIJxzzkXlCcI551xUniCcc85F5b2YXKmbNGlS2CE452LgJQjnnHNReYJwzjkXlScI55xzUXmCcM45F5UnCOecc1F5gnDOOReVJwjnnHNReYJwzjkXlScI55xzUR00NwwSkTXAz/uxidpAZgmFU5ISNS5I3NgSNS5I3NgSNS5I3NgSNS4oXmxHq2qdaDMOmgSxv0QkraC7KoUpUeOCxI0tUeOCxI0tUeOCxI0tUeOCkovNq5icc85F5QnCOedcVJ4gcr0QdgAFSNS4IHFjS9S4IHFjS9S4IHFjS9S4oIRi8zYI55xzUXkJwjnnXFSeIJxzzkV1yCcIEekuIukislhE7gg5lldEZLWIzIuYVlNEvhKRRcHfGiHE1VBEJorIAhGZLyJ/TqDYKorINBGZE8R2fzC9iYj8L/heR4lI+dKOLYgjSURmicgnCRbXUhH5TkRmi0haMC0Rvs/qIjJGRBaKyPcicmqCxNUs+KxyHhtF5OYEiW1Q8NufJyLvBP8TJfI7O6QThIgkAc8APYAWQD8RaRFiSCOA7vmm3QFMUNWmwITgdWnbBdyiqi2AU4Drg88pEWLbAZyjqq2BZKC7iJwCPAo8parHAb8BfwohNoA/A99HvE6UuADOVtXkiP7yifB9/h34XFWbA62xzy70uFQ1PfiskoF2wFbgg7BjE5H6wE1AiqqeCCQBl1JSvzNVPWQfwKnAFxGvhwBDQo6pMTAv4nU6UDd4XhdIT4DP7SPg3ESLDagMzAROxq4iLRvtey7FeBpgB41zgE8ASYS4gn0vBWrnmxbq9wlUA34i6DyTKHFFibMrMCURYgPqA8uAmkDZ4HfWraR+Z4d0CYLcDzdHRjAtkRypqiuD578CR4YZjIg0BtoA/yNBYguqcWYDq4GvgB+B9aq6K1gkrO91OHAbkB28rpUgcQEo8KWIzBCRAcG0sL/PJsAa4NWgWu4lETksAeLK71LgneB5qLGp6nJgGPALsBLYAMyghH5nh3qCOKConQ6E1i9ZRKoA7wE3q+rGyHlhxqaqu9WK/g2ADkDzMOKIJCLnA6tVdUbYsRTgDFVti1WvXi8iZ0XODOn7LAu0Bf6lqm2ALeSrskmA/4HyQC/g3fzzwogtaPO4EEuu9YDD2Luaep8d6gliOdAw4nWDYFoiWSUidQGCv6vDCEJEymHJ4S1VfT+RYsuhquuBiViRurqIlA1mhfG9ng70EpGlwEismunvCRAXsOfME1VdjdWldyD87zMDyFDV/wWvx2AJI+y4IvUAZqrqquB12LF1AX5S1TWqmgW8j/32SuR3dqgniOlA06DFvzxWdBwbckz5jQX6B8/7Y/X/pUpEBHgZ+F5Vn0yw2OqISPXgeSWsbeR7LFH0CSs2VR2iqg1UtTH2u/paVf8QdlwAInKYiFTNeY7Vqc8j5O9TVX8FlolIs2BSZ2BB2HHl04/c6iUIP7ZfgFNEpHLwf5rzmZXM7yzMxp5EeAA9gR+weuu7Qo7lHaweMQs7m/oTVm89AVgEjAdqhhDXGVjReS4wO3j0TJDYTgJmBbHNA+4Jph8DTAMWY9UBFUL8XjsBnyRKXEEMc4LH/JzffYJ8n8lAWvB9fgjUSIS4gtgOA9YC1SKmhR4bcD+wMPj9vwFUKKnfmQ+14ZxzLqpDvYrJOedcATxBOOeci8oThHPOuag8QTjnnIvKE4RzzrmoPEE4VwQR2Z1vJM8SG5BNRBpLxOi9ziWSskUv4twhb5vaUB7OHVK8BOHcPgruqfBYcF+FaSJyXDC9sYh8LSJzRWSCiDQKph8pIh8E966YIyKnBZtKEpEXgzH9vwyuCEdEbhK7B8dcERkZ0tt0hzBPEM4VrVK+Kqa+EfM2qGor4J/Y6K0A/wBeU9WTgLeAp4PpTwP/Vrt3RVvsKmaApsAzqtoSWA9cHEy/A2gTbGdgvN6ccwXxK6mdK4KIbFbVKlGmL8VuVrQkGMzwV1WtJSKZ2D0CsoLpK1W1toisARqo6o6IbTQGvlK74QwicjtQTlUfFJHPgc3YkBMfqurmOL9V5/LwEoRz+0cLeF4cOyKe7ya3bfA87I6HbYHpEaNzOlcqPEE4t3/6RvydGjz/FhvBFeAPwDfB8wnAtbDnJkfVCtqoiJQBGqrqROB27G5re5VinIsnPyNxrmiVgjvW5fhcVXO6utYQkblYKaBfMO1G7K5og7E7pP0xmP5n4AUR+RNWUrgWG703miTgzSCJCPC02v0unCs13gbh3D4K2iBSVDUz7FiciwevYnLOOReVlyCcc85F5SUI55xzUXmCcM45F5UnCOecc1F5gnDOOReVJwjnnHNR/T+Xf/1SoERVHQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUvTzo7ptjYz","executionInfo":{"status":"ok","timestamp":1622482153129,"user_tz":240,"elapsed":13,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"c5a50bf0-aed8-45a5-e238-e1d33509ab68"},"source":["## n_users=14811\n","## n_beers=52583\n","## device:gpu\n","## hidden_size=150\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","best_model = NeuMF(gmf_model=gmf_model, mlp_model=mlp_model, n_input=30, device=device)\n","#torch.save(best_state_dict, \"/content/drive/MyDrive/CS247/Models/checkpoints/neu_mf.pth\")\n","best_model.load_state_dict(best_state_dict['model_state_dict'])"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"GtaIsbzItqTb","executionInfo":{"status":"ok","timestamp":1622482153129,"user_tz":240,"elapsed":6,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"4aa419f6-db5e-4695-d55c-8753be0c8709"},"source":["#label_pred = 'pred_y'\n","best_model.to(device)\n","best_model.eval()\n","test_data[label_pred] = (best_model.predict(test_data)>0.5).cpu().detach().numpy().astype(int)\n","test_data"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>beer_id</th>\n","      <th>review_overall</th>\n","      <th>pred_y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12</td>\n","      <td>5892</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17</td>\n","      <td>5892</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>300822</th>\n","      <td>14731</td>\n","      <td>14334</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>300823</th>\n","      <td>14776</td>\n","      <td>42943</td>\n","      <td>4.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>300824</th>\n","      <td>14780</td>\n","      <td>22042</td>\n","      <td>4.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>300825</th>\n","      <td>14785</td>\n","      <td>36467</td>\n","      <td>3.5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>300826</th>\n","      <td>14790</td>\n","      <td>46338</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>300827 rows × 4 columns</p>\n","</div>"],"text/plain":["        user_id  beer_id  review_overall  pred_y\n","0             0     5892             4.0       1\n","1             2     5892             4.0       1\n","2             9     5892             4.0       1\n","3            12     5892             4.0       0\n","4            17     5892             5.0       1\n","...         ...      ...             ...     ...\n","300822    14731    14334             2.5       1\n","300823    14776    42943             4.5       1\n","300824    14780    22042             4.5       1\n","300825    14785    36467             3.5       1\n","300826    14790    46338             2.0       1\n","\n","[300827 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtsTIk1otsPY","executionInfo":{"status":"ok","timestamp":1622482194209,"user_tz":240,"elapsed":41085,"user":{"displayName":"TONG WU","photoUrl":"","userId":"01876243748839276301"}},"outputId":"63dee220-4ae3-4817-ad44-ead671aa798a"},"source":["#threshold=4\n","#k=5\n","test_prec, test_recall = precision_recall_at_k(test_data, label_pred=label_pred, threshold=threshold, k=k)\n","# Precision and recall can then be averaged over all users\n","print(\"precision at 5 for test set: \", sum(prec for prec in test_prec.values()) / len(test_prec))\n","print(\"recall at 5 for test set:\", sum(rec for rec in test_recall.values()) / len(test_prec))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["precision at 5 for test set:  0.6802134702425344\n","recall at 5 for test set: 0.5491672346200788\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aOiIJM6WcR25"},"source":[""],"execution_count":null,"outputs":[]}]}